1(b).2.1 : Measures Similarity Dissimilarity | STAT 897D Skip Content Eberly College Science STAT 897D Applied Data Mining Statistical Learning Home » Lesson 1 ( b ): Exploratory Data Analysis ( EDA ) » 1(b ) .2 - Numerical Summarization 1(b).2.1 : Measures Similarity Dissimilarity Printer - friendly version Similarity Dissimilarity Distance similarity measures essential solve pattern recognition problems classification clustering . distance / similarity measures available literature compare data distributions .    names suggest , similarity measures close distributions . multivariate data complex summary methods developed answer question . Similarity Measure Numerical measure alike data objects . falls 0 ( similarity ) 1 ( complete similarity ) . Dissimilarity Measure Numerical measure different data objects . Range 0 ( objects alike ) ∞ ( objects different ) . Proximity refers similarity dissimilarity . Similarity / Dissimilarity Simple Attributes , p q attribute values data objects . Attribute Type Similarity Dissimilarity Nominal \(s=\begin{cases } 1 & \text { } p = q \\ 0 & \text { } p\neq q \end{cases}\ ) \(d=\begin{cases } 0 & \text { } p = q \\ 1 & \text { } p\neq q \end{cases}\ ) Ordinal \(s=1-\frac{\left \| p - q \right \|}{n-1}\ ) ( values mapped integer 0 n-1 , n number values ) \(d=\frac{\left \| p - q \right \|}{n-1}\ ) Interval Ratio \(s=1-\left \| p - q \right \| ,    s=\frac{1}{1+\left \| p - q \right \|}\ ) \(d=\left \| p - q \right \|\ ) Common Properties Dissimilarity Measures Distance , Euclidean distance , dissimilarity measure known properties : d ( p , q ) ≥ 0 p q , d ( p , q ) = 0 p = q , d ( p , q ) = d(q , p ) p q , d ( p , r ) ≤ d ( p , q ) + d ( q , r ) p , q , r , d ( p , q ) distance ( dissimilarity ) points ( data objects ) , p q . distance satisfies properties called metric .Following list common distance measures compare multivariate data . assume attributes continuous . Euclidean Distance Assume measurements x ik , = 1 , … , N , variables k = 1 , … , p ( called attributes ) . Euclidean distance th j th objects \[d_E(i , j)=\left(\sum_{k=1}^{p}\left(x_{ik}-x_{jk }    \right ) ^2\right)^\frac{1}{2}\ ] pair ( , j ) observations . weighted Euclidean distance \[d_{WE}(i , j)=\left(\sum_{k=1}^{p}W_k\left(x_{ik}-x_{jk }    \right ) ^2\right)^\frac{1}{2}\ ] scales attributes differ substantially , standardization necessary . Minkowski Distance Minkowski distance generalization Euclidean distance . measurement , x ik , = 1 , … , N ,    k = 1 , … , p , Minkowski distance \[d_M(i , j)=\left(\sum_{k=1}^{p}\left | x_{ik}-x_{jk }    \right | ^ \lambda \right)^\frac{1}{\lambda } , \ ] λ ≥ 1 .    called     L λ metric . λ = 1 : L 1 metric , Manhattan City - block distance . λ = 2 : L 2 metric , Euclidean distance . λ → ∞ : L ∞ metric , Supremum distance . \ [    \lim{\lambda \to \infty}=\left ( \sum_{k=1}^{p}\left | x_{ik}-x_{jk }    \right | ^ \lambda \right ) ^\frac{1}{\lambda }    = \text{max}\left ( \left | x_{i1}-x_{j1}\right|    , ... ,    \left | x_{ip}-x_{jp}\right| \right ) \ ] Note λ p different parameters . Dimension data matrix remains finite . Mahalanobis Distance Let X N × p matrix . th row X \[x_{i}^{T}=\left ( x_{i1 } , ... , x_{ip } \right)\ ] Mahalanobis distance \[d_{MH}(i , j)=\left ( \left ( x_i - x_j\right)^T \Sigma^{-1 } \left ( x_i - x_j\right)\right)^\frac{1}{2}\ ] ∑ p×p sample covariance matrix . Self - check Think ! Calculate answers questions click icon left reveal answer . 1 .     \(X= \begin{pmatrix } 1 & 3 & 1 & 2 & 4\\ 1 & 2 & 1 & 2 & 1\\ 2 & 2 & 2 & 2 & 2 \end{pmatrix}\ ) . Calculate Euclidan distances . Calculate Minkowski distances ( λ=1 λ→∞ cases ) . Euclidean distances : d E ( 1,2 ) = ( ( 1 - 1 ) 2 + ( 3 - 2 ) 2 + ( 1 - 1 ) 2 + ( 2 - 2 ) 2 + ( 4 - 1 ) 2 ) 1/2 = 3.162 . d E ( 1,3 ) = ( ( 1 - 2 ) 2 + ( 3 - 2 ) 2 + ( 1 - 2 ) 2 + ( 2 - 2 ) 2 + ( 4 - 2 ) 2 ) 1/2 = 2.646 . d E ( 2,3 ) = ( ( 1 - 2 ) 2 + ( 2 - 2 ) 2 + ( 1 - 2 ) 2 + ( 2 - 2 ) 2 + ( 1 - 2 ) 2 ) 1/2 = 1.732 . Minkowski distances ( λ=1 ) : d M ( 1,2 ) = |1 - 1| + |3 - 2| + |1 - 1| + |2 - 2| + |4 - 1| = 4 . d M ( 1,3 ) = |1 - 2| + |3 - 2| + |1 - 2| + |2 - 2| + |4 - 2| = 5 . d M ( 2,3 ) = |1 - 2| + |2 - 2| + |1 - 2| + |2 - 2| + |1 - 2| = 3 . Minkowski distances ( λ→∞ ) : d M ( 1,2 ) = max(|1 - 1| , |3 - 2| , |1 - 1| , |2 - 2| , |4 - 1| ) = 3 . d M ( 1,3 ) = 2 d M ( 2,3 ) = 1 . 2 .       \(X= \begin{pmatrix } 2 & 3 \\ 10 & 7 \\ 3 & 2 \end{pmatrix}\ ) . Calculate Minkowski distance ( λ = 1 , λ = 2 , λ → ∞ cases ) second objects . Calculate Mahalanobis distance second objects . Minkowski distance : λ = 1 . d M ( 1,2 ) = |2 - 10| + |3 - 7| = 12 . λ = 2 . d M ( 1,2 ) = d E ( 1,2)=((2 - 10 ) 2 + ( 3 - 7 ) 2 ) 1/2 = 8.944 . λ → ∞. d M ( 1,2 ) = max(|2 - 10| , |3 - 7| ) = 8 . Mahalanobis distance : d MH ( 1,2 ) = 2 . Common Properties Similarity Measures Similarities known properties : s ( p , q ) = 1 ( maximum similarity ) p = q , s ( p , q ) = s ( q , p ) p q , s ( p , q ) similarity data objects , p q . Similarity  Binary Variables similarity distance measures appropriate continuous variables . , binary variables different approach necessary . Simple Matching Jaccard Coefficients Simple matching coefficient = ( n 1,1 + n 0,0 ) / ( n 1,1 + n 1,0 + n 0,1 + n 0,0 ) . Jaccard coefficient = n 1,1 / ( n 1,1 + n 1,0 + n 0,1 ) . Self - check Think ! Calculate answers question click icon left reveal answer . 1 . Given data : p = 1 0 0 0 0 0 0 0 0 0 q = 0 0 0 0 0 0 1 0 0 1 frequency table Calculate Simple matching coefficient Jaccard coefficient . Simple matching coefficient = ( 0 + 7 ) / ( 0 + 1 + 2 + 7 ) = 0.7 . Jaccard coefficient = 0 / ( 0 + 1 + 2 ) = 0 . ‹ 1(b ) .2 - Numerical Summarization 1(b ) .3 - Visualization › Printer - friendly version Navigation Start ! Welcome STAT 897D - Applied Data Mining Statistical Learning Search Course Materials Lessons Lesson 1(a ): Introduction Data Mining Lesson 1 ( b ): Exploratory Data Analysis ( EDA ) 1(b ) .1 - Data 1(b ) .2 - Numerical Summarization 1(b).2.1 : Measures Similarity Dissimilarity 1(b ) .3 - Visualization 1(b ) .4 - R Scripts Lesson 2 : Statistical Learning Model Selection Lesson 3 : Linear Regression Lesson 4 : Variable Selection Lesson 5 : Regression Shrinkage Methods Lesson 6 : Principal Components Analysis Lesson 7 : Dimension Reduction Methods Lesson 8 : Modeling Non - linear Relationships Lesson 9 : Classification Lesson 10 : Support Vector Machines Lesson 11 : Tree - based Methods Lesson 12 : Cluster Analysis Resources Analysis German Credit Data Analysis Wine Quality Data Analysis Classification Data Final Project - Sample Work Copyright © 2018 Pennsylvania State University Privacy Legal Statements Contact Department Statistics Online Programs