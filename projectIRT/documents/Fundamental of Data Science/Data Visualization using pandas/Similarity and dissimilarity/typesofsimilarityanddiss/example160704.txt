17 types similarity dissimilarity measures data science . | Mahmoud Harmouch | TDS Archive | Medium Open app Sign Sign Write Sign Sign TDS Archive ¬∑ archive data science , data analytics , data engineering , machine learning , artificial intelligence writing Data Science Medium publication . 17 types similarity dissimilarity measures data science . following article explains methods computing distances showing instances daily lives . Additionally , introduce pydist2 package . Mahmoud Harmouch Follow 25 min read ¬∑ Mar 13 , 2021 -- 18 Listen Share ML metrics . Inspired Maarten Grootendorst . ‚Äú Royal Road Geometry . ‚Äù ‚Äî Euclid Quick note : written visualized created author specified . Illustrations equations generated tools like Matplotlib , Tex , Scipy , Numpy edited GIMP . üÑ∞ . Introduction : ‚Ä¢ Similarity dissimilarity : data science , similarity measure way measuring data samples related closed . hand , dissimilarity measure tell data objects distinct . , terms clustering similar data samples grouped cluster . data samples grouped different ones . classification(e.g . KNN ) , data objects labeled based features ‚Äô similarity . example talk dissimilar outliers compared data samples(e.g . , anomaly detection ) . similarity measure usually expressed numerical value : gets higher data samples alike . expressed number zero conversion : zero means low similarity(the data objects dissimilar ) . means high similarity(the data objects similar ) . Let example data point contains input feature . considered simplest example dissimilarity data points , B , C. data sample single value axis(because input feature ) ; let denote x - axis . Let points , A(0.5 ) , B(1 ) , C(30 ) . tell , B close contrast C. , similarity B higher C B C. terms , B strong correlation . , smaller distance , larger similarity . ‚Ä¢ Metric : given distance(e.g . dissimilarity ) meant metric satisfies following conditions : 1- Non - negativity : d(p , q ) ‚â• 0 , distinct observations p q. 2- Symmetry : d(p , q ) = d(q , p ) p q. 3- Triangle Inequality : d(p , q ) ‚â§ d(p , r ) + d(r , q ) p , q , r. 4- d(p , q ) = 0 p = q. Distance measures fundamental principle classification , like k - nearest neighbor classifier algorithm , measures dissimilarity given data samples . Additionally , choosing distance metric strong influence performance classifier . , way compute distances objects play crucial role classifier algorithm performance . üÑ± . Distance Functions : technique measure distances depends particular situation working . instance , areas , euclidean distance optimal useful computing distances . applications require sophisticated approach calculating distances points observations like cosine distance . following enumerated list represents methods computing distances pair data points . ‚ì™. L2 norm , Euclidean distance . Euclidean Contours . common distance function numeric attributes features Euclidean distance defined following formula : Euclidean distance points n - dimensional space . know , distance metric presents - known properties , like symmetrical , differentiable , convex , spherical ‚Ä¶ 2 - dimensional space , previous formula expressed : Euclidean distance points 2 - dimensional space . equal length hypotenuse right - angle triangle . , Euclidean distance metric satisfies criterion , following illustration shows . Euclidean distance satisfies conditions metric . Furthermore , distance calculated formula represents smallest distance pair points . words , shortest path point point B(2 - D Cartesian coordinate system ) , following figure illustrates : Euclidean distance shortest path(Excluding case wormhole quantum world ) . , useful use formula want compute distance 2 points absence obstacles pathway . considered situations want compute euclidean distance ; instead , want use metrics like Manhattan distance , explained later article . scenario euclidean distance fails useful information plane flight path follows earth curvature , straight line(unless earth flat , ! ) . , going , let explain use euclidean distance context machine learning . famous classification algorithms , KNN algorithm , benefit euclidean distance classify data . demonstrate KNN works euclidean metric , popular iris dataset Scipy package chosen . know , dataset contains kinds flowers : Iris - Setosa , Iris - Versicolor , Iris - Virginica , having following features : sepal length , sepal width , petal length , petal width . , 4 - dimensional space data point represented . Iris dataset types flowers features ‚Äô space . simplicity demonstration purposes , let choose features : petal length , petal width , excluding Iris - virginica data . way , plot data points 2 - D space x - axis y - axis represent petal length petal width , respectively . Training dataset . data point came label : Iris - Setosa Iris - versicolor(0 1 dataset ) . , dataset KNN classification supervised ML algorithm nature . Let assume ML model(KNN k = 4 ) trained dataset selected input features data points , previous graph shows . point , looks great , KNN classifier ready classify new data point . , need way let model decide new data point classified . Predict label new data point . thinking , euclidean distance chosen let trained data point vote new data sample fit : Iris - Setosa Iris - versicolor . , euclidean distance calculated new data point point training data , following figure shows : k = 4 , KNN classifier requires chose smallest distances , represents distance new point following points : point1 , point5 , point8 , point9 graph shows : neighbors voted Iris - Setosa . , new data sample classified Iris - Setosa . analogy , imagine higher dimensions classifiers . Hopefully , idea ! stated earlier , domain requires specific way computing distances . progress article , find meant stating . ‚ûÄ. Squared Euclidean distance . Computing distances approach avoids need use squared root function . reflects , SED equal euclidean distance squared . , SED reduce computational work calculating distances observations . instance , clustering , classification , image processing , domains . Squared Euclidean distance points n - D space . ‚ë°. L1 norm , City Block , Manhattan , taxicab distance . Manhattan Contours . metric useful measuring distance streets given city , distance measured terms number blocks separate different places . instance , according following image , distance point point B roughly equal 4 blocks . Manhattan distance real world method created solve computing distance source destination given city nearly impossible straight line buildings grouped grid blocks straight pathway . City Block . distance B euclidean . , notice distance useful . instance , need useful distance estimate travel time long drive . Instead , help shortest path streets . depends situation define use distance . n - dimensional space , Manhattan distance expressed : Manhattan distance points n - D space . 2 - dimensional grid , previous formula written : Manhattan distance points 2 - D space . Recall previous KNN example , Computing manhattan distance new data point training data produce following values : KNN classification Manhattan distance(tie ! ) , data points voted Iris - Setosa , points voted Iris - versicolor , means tie . Manhattan distance : tie ! think encountered problem . intuitive solution changing value k , decreasing k larger , increasing , . , different behavior KNN classifier previous solutions . instance , example , k=4 . Changing k=3 result following values : Decreasing k . flower classified Iris - versicolor . manner , Changing k=5 result following values : Increasing k . flower classified Iris - Setosa . , decide need increase decrease value k. , argue change metric measure constraint problem . example , computing euclidean distance solve : Changing distance metric break tie . flower strongly classified Iris - Setosa . opinion , change Manhattan distance use value k , adding new dimension feature , available , break tie . instance , adding sepal width new dimension lead following results : Adding new feature model . flower classified Iris - Versicolor . goes plot 3 - D space x - axis , y - axis , z - axis represent sepal width , petal length , petal width , respectively : 3 - D plot Iris dataset . Computing Manhattan distance computationally faster previous methods . formula shows , requires additions subtractions , turns faster computing square root power . played chess , Manhattan distance bishops order horizontal vertical blocks color : Bishop uses Manhattan distance(if , imagine rotating chessboard 45 ¬∞ ) . terms , number moves(distance ) required bishop red squares equal Manhattan distance , . Aside , Manhattan distance preferred Euclidean distance data present outliers . , L1 - norm gives sparse estimation l2 - norm . , L1 - norm L2 - norm commonly Regularization neural network minimize weights zero values , like lasso regression . Forms constraint regions lasso ridge regression(Source : Wikipedia ) . figure , L1 - norm tries zero W1 weight minimize . , L2 - norm tries minimize W1 W2 weights(like W1 = W2 ) . meantime , want article deep dive regularization main goal explain common distance functions stating usage making digestible . , let . ‚ë¢. Canberra distance . weighted version manhattan distance Clustering , like Fuzzy Clustering , classification , computer security , ham / spam detection systems . robust outliers contrast previous metric . Canberra distance . ‚ë£. L‚àû norm , Chebyshev distance , maximum distance . Chebyshev contours . Chebyshev distance n - D observations vectors equal maximum absolute value variations data samples ‚Äô coordinates . 2 - D world , Chebyshev distance data points determined sum absolute differences 2 - dimensional coordinates . Chebyshev distance points P Q defined : Chebyshev distance Chebyshev distance metric satisfies conditions metric . Chebyshev distance satisfies conditions metric . , wondering min function metric ! min function metric counterexample(e.g . horizontal vertical line ) d(A , B ) = 0 ! = B. , equal zero = B ! ! ! ! use cases think uses Chebyshev metric trading stocks , cryptocurrencies features like volume , Bid , Ask ‚Ä¶ instance , need find way tells cryptocurrency big gap rewards losses . turns Chebyshev distance good fit particular situation . common scenario Chebyshev distance chessboard number moves king , queen , equal distance order reach neighbor square following figure shows : king uses Chebyshev distance . queen uses Chebyshev distance squares . ‚ë§. Lp norm , Minkowski distance . Minkowski contours different values p. Minkowski distance generalization previous distance metrics : Euclidean , Manhattan , Chebyshev . defined distance observations n - D space following formula demonstrate : Minkowski distance . P , Q given n - D points , p represents Minkowski metric . particular value p , derive following metrics : p = 1 : Manhattan distance . p = 2 : Euclidean distance . p ‚Üí + ‚àû : Chebyshev distance , logical OR(point D = B = 1 1 = 1 ) . p ‚Üí 0 : logical AND(point C = B = Zero ) . p ‚Üí -‚àû : min distance(the symmetric point D ) . ‚ë•. Cosine distance . metric widely text mining , natural language processing , information retrieval systems . instance , measure similarity given documents . identify spam ham messages based length message . Cosine distance measured follows : Cosine distance . P Q represent given points . points represent frequencies words documents explained following example . Let , instance , documents contain following phrases : Document : ‚Äú love drink coffee morning . ‚Äù Document B : ‚Äú like drink coffee . ‚Äù Document C : ‚Äú friend work coffee shop hometown . tells good jokes morning . like begin day drink cup tea . ‚Äù Computing frequency , occurrence word result following : words ‚Äô frequencies . computing number occurrences , knew priori documents B similar meaning : ‚Äú love drink coffee . ‚Äù , document C contains words document dissimilar meaning , frequencies table . solve issue , need compute cosine similarity find similar . hand , illustrate information retrieval , search engine , works . Think document query(short message ) given source(image , text , video ‚Ä¶ ) document C web page needs fetched returned response query . , euclidean distance fail correct distance short large documents huge situation . cosine similarity formula compute difference documents terms directions magnitude . illustrate , let following documents : Document : ‚Äú Bitcoin Bitcoin Bitcoin Money ‚Äù Document B : ‚Äú Money Money Bitcoin Bitcoin ‚Äù let denote word ‚Äú Bitcoin ‚Äù x - axis word ‚Äú Money ‚Äù y - axis . means document represented vector A(3,1 ) document B B(2,2 ) . Computing cosine similarity result following value : Computing cosine similarity Cosine_Similarity = 0.894 means documents B , similar . cos(angle ) large(close ) means angle small(26.6 ¬∞ ) , documents B closed . , interpret value Cosine Similarity percentage . instance , value 0.894 mean document 89.4 % , similar B. means documents B similar , know percentage ! threshold value . words , interpret value Cosine Similarity follows : larger gets , likely documents B similar , vice versa . Let example A(1 , 11 ) B(22 , 3 ) Computing cosine similarity , euclidean distance large number like 22.4 , tell relative similarity vectors . hand , cosine similarity works higher dimensions . interesting application cosine similarity OpenPose project . Congrats üéÜ ! halfway üèÅ . üèÉ ! ‚ë¶. Pearson Correlation distance . Correlation distance quantifies strength linear , monotonic relationship attributes . Furthermore , uses covariance value initial computational step . , covariance hard interpret data close far line representing trend measurements . correlation means , let Iris dataset plot Iris - Setosa samples relationship features : petal length petal width . Iris - Setosa samples features ‚Äô measurements . sample mean values variance features flower samples estimated , figure shows . Generally speaking , flowers relatively low petal length values relatively low values petal width . flowers relatively high values petal length relatively high values petal width . Additionally , summarize relationship line . sample mean variance estimation . line represents positive trend values petal length petal width increase . Covariance value classify types relationships : types correlations . correlation distance calculated following formula : Correlation distance numerator represents covariance value observations , denominator represents square root variance feature . Let simple example demonstrate compute formula . red blue points following coordinates , respectively : A(1.2 , 0.6 ) B ( 3.0 , 1.2 ) . estimated sample means measurements equals : point metric correlation mean causation . instance , iris - Setosa relatively small petal length value mean petal width value small . sufficient Necessary Condition ! small petal length probably contributes small petal width , cause ! ‚ëß. Spearman correlation . Like Pearson correlation , Spearman correlation dealing bivariate analysis . , unlike Pearson correlation , Spearman correlation variables rank - ordered . categorical numerical attributes . Correlation matrix iris dataset . Spearman correlation index calculated following formula : Spearman correlation coefficient . Spearman correlation hypothesis testing . ‚ë® . Mahalanobis distance . metric measure multivariate statistical testing euclidean distance fails real distance observations . measures far away data point distribution . points value ED mean . shown previous image , red blue points Euclidean distance mean . , fall region cluster : red point likely similar data set . blue considered outlier far away line represents direction greatest variability dataset(major axis regression ) . , Mahalanobis metric introduced solve issue . Mahalanobis metric tries decrease covariance features attributes sense rescale previous plot new axes . new axes represent eigenvectors like eigenvector previously shown . direction eigenvector greatly influences data classification largest eigenvalue . Furthermore , dataset spread direction perpendicular direction . technique , shrink dataset direction rotate mean(PCA ) . use euclidean distance , gives different distances mean previous data points . Mahalanobis metric . Mahalanobis distance objects P Q. C represents covariance matrix attributes features . demonstrate formula usage , let compute distance A(1.2 , 0.6 ) B ( 3.0 , 1.2 ) previous example correlation distance section . Let evaluate covariance matrix , defined follows : Covariance matrix 2d space Cov[P , P ] = Var[P ] Cov[Q , Q]= Var[Q ] , Covariance formula features . , Mahalanobis distance objects B calculated follows : Mahalanobis distance example . addition use cases , Mahalanobis distance Hotelling t - square test . ‚ë©. Standardized Euclidian distance . Standardization normalization technique preprocessing stage building machine learning model . dataset presents high difference minimum maximum ranges features . scale distance affect ML model clustering data , leading wrong interpretation . instance , let situation different features present large difference range variations . example , let feature varies 0.1 2 feature goes 50 200 . Computing distance values second feature dominant , leading incorrect results . terms , euclidean distance highly influenced attributes largest values . standardization necessity order let features contribute equal manner . transforming variables variance equal centralize features mean like following formula : z - score standardization . standardized Euclidean distance expressed follows : standardized euclidian distance . apply formula compute distance B. ‚ë™. Chi - square distance : Chi - square distance commonly computer vision texture analysis order find ( dis)similarities normalized histograms , known ‚Äú Histogram matching ‚Äù . Histogram matching . Source : Wikipedia face recognition algorithm great example uses metric order compare histograms . instance , prediction step new face , model computes histogram newly captured image , compared saved histograms(often stored .yaml file ) , tries find best match . comparison computing Chi - square distance pair histograms n bins . Chi - square distance know , formula different Chi - square statistics test standard normal distributions , decide retain reject null hypothesis following formula : Chi - square test formula . O E represent observed expected data values , respectively . illustrate , let survey 1000 persons test given vaccine effect significant difference based gender . , person categories : 1- Male effects . 2- Male effects . 3- Female effects . 4- Female effects . null hypothesis : significant difference effects genders . order retain reject hypothesis , compute Chi - square test value following data : Collected data . plugging values Chi - square test formula , 1.7288 . Chi - square table degree freedom equal 1 , probability 0.2 0.1 > 0.05 ‚Üí retain null hypothesis . Note degree freedom = ( number columns -1 ) x ( number rows -1 ) , wanted quick refresher hypothesis testing ; hope find helpful . , let things moving . üèÉ üèÉ üèÉ ‚ë´. Jensen - Shannon distance . Jensen - Shannon distance computes distance probability distributions . uses Kullback Leibler divergence(The relative entropy ) formula order find distance . Jensen - Shannon distance . R midpoint P Q. , quick note interpret value entropy : Low entropy event means high knowing event occur ; terms , surprised event going happen , highly confident happen . analogy High entropy . hand , Kullback Leibler divergence distance metric symmetric : D(P || Q ) ! = D(Q || P ) . ‚ë¨. Levenshtein distance metric measuring similarity strings . equal minimum number operations required transform given string . types operations : Substitution . Insertion Deletion Levenshtein distance , substitution cost units operations . instance , let strings s= ‚Äú Bitcoin ‚Äù t = ‚Äú Altcoin ‚Äù . s t , need substitutions letters ‚Äú B ‚Äù ‚Äú ‚Äù letters ‚Äú ‚Äù ‚Äú l ‚Äù . , d(t , s ) = 2 * 2 = 4 . use cases Levenshtein distance like spam filtering , computational biology , Elastic search , . ‚ë≠. Hamming distance . hamming distance equal number digits codewords length differ . binary world , equal number different bits binary messages . example , hamming distance messages calculated : Hamming distance . notice , looks like manhattan distance context categorical data . messages length 2 bits , formula represents number edges separate given binary messages . equal . 2 - bits codewords . manner , messages length 3 bits , formula represents number edges separates given binary messages . equal . 3 - bits codewords . Let example illustrate hamming distance calculated : H(100001 , 010001 ) = 2 H(110 , 111 ) = 1 messages contains zeros , hamming distance called hamming weight equal number non - zero digits given message . case , equal total number ones . H(110111,000000 ) = W ( 110111 ) = 5 hamming distance detect correct , possible , errors received messages transmitted unreliable noisy channel . ‚ëÆ. Jaccard / Tanimoto distance . metric measure similarity sets data . argue order measure similarity compute size(cardinality , number elements . ) intersection given sets . , number common elements tell relative compared sets ‚Äô size . intuition Jaccard coefficient . Jaccard proposes , order measure similarity , need divide size intersection size union sets data . Jaccard distance . Jaccard distance complementary Jaccard coefficient measures dissimilarity data sets calculated : Jaccard distance . following illustration explains formula non - binary data . Jaccard index example . binary attributes , Jaccard similarity calculated following formula : Jaccard index binary data . Jaccard index useful domains like semantic segmentation , text mining , E - Commerce , recommendation systems . thinking : ‚Äú Ok , mentioned earlier cosine distance text mining . prefer use metric given clustering algorithm ? difference metrics ? ‚Äù Glad asked question . order answer , need compare term formulas . Jaccard cosine formulas . difference formulas denominator term . Instead computing union size sets Jaccard , computing magnitude dot product P Q. Instead adding terms Jaccard formula denominator ; computing product cosine formula . know interpretation . far know , dot product gives vector goes direction . , add , grateful read thoughts comments . ‚ëØ. S√∏rensen ‚Äì Dice . S√∏rensen ‚Äì Dice distance statistical metric measure similarity sets data . defined times size intersection P Q , divided sum elements data set P Q. S√∏rensen ‚Äì Dice coefficient . Like Jaccard , similarity values range zero . , unlike Jaccard , dissimilarity measure metric satisfy triangle inequality condition . S√∏rensen ‚Äì Dice lexicography , image segmentation , applications . üÑ≤ . Pydist2 . past weeks , researching similarity dissimilarity measures , thought fun / great idea reimplement types measures python coding practice . took inspiration Piotr Dollar toolbox repo written Matlab found MathWorks website . , pydist2 python package , 1:1 code adoption pdist pdist2 Matlab functions , computing distance observations . list methods measuring distance currently supported pydist2 available read docs . üÑ≥ . Conclusion . Hooray ! ! ! reached end article . Hopefully , overwhelmed content point , tried concise clear possible . article , learned different types metrics data science applications areas . , writing article enjoyable experience data science journey . stress fact spending countless hours researching exploring mathematical branch data science machine learning practicing coding skills writing python package : pydist2 . want contribute project , welcoming pull requests pydist2 repo . Checkout tutorial want contribute . noticed mistakes reading article , mention comments order improve content . suggestions , drop message LinkedIn send email . want use article , cite : Mahmoud Harmouch , 17 types similarity dissimilarity measures data science , medium.com . Mar-14 - 2021 analytical skills convinced want , following and/or sharing article help gain exposure data science community spread useful information . today blog . Good luck data science journey ! Thanks reading , ! Machine Learning Data Science Python Mathematics Data -- -- 18 Follow Published TDS Archive 820 K Followers ¬∑ published Feb 3 , 2025 archive data science , data analytics , data engineering , machine learning , artificial intelligence writing Data Science Medium publication . Follow Follow Written Mahmoud Harmouch 858 Followers ¬∑ 0 Following Senior Blockchain Rust Enjoyer GigaDAO - occasionally write articles data science , machine learning Blockchain Rust - Currently Writing Books Follow Responses ( 18 ) responses Help Status Careers Press Blog Privacy Rules Terms Text speech