Data Preprocessing Steps Machine Learning Python ( 1 ) | Learn Nas | Women Technology | Medium Open app Sign Sign Write Sign Sign Women Technology · Women Tech publication highlight women STEM , accomplishments , career lessons , stories . Data Preprocessing Steps Machine Learning Python ( 1 ) Learn Nas Follow 14 min read · Sep 30 , 2023 -- 4 Listen Share Data Preprocessing , recognized Data Preparation Data Cleaning , encompasses practice identifying rectifying erroneous misleading records dataset . involves pinpointing flawed , incomplete , irrelevant segments data subsequently modifying , substituting , eliminating impure coarse data [ 1 ] . Data Preprocessing techniques adapted train AI models , including machine learning models . techniques generally early stages ensure accurate results [ 2 ] . aware data preprocessing comprehensive term covering wide range tasks , spanning formatting data creating features , depending nature AI project . preparatory phase enhances overall quality data streamlines modelling process , ultimately leading reliable accurate predictive models . article delves vital role Data Preprocessing plays context Machine Learning , shedding light aspects emphasizing necessity achieving meaningful impactful results . important ? significance Data Preprocessing Machine Learning overstated , forms cornerstone successful data analysis machine learning endeavour . realm data - driven technologies , quality suitability data directly influence outcomes effectiveness machine learning models . Data Preprocessing involves series steps : Data Collection Data Cleaning Data Transformation Feature Engineering : Scaling , Normalization Standardization Feature Selection Handling Imbalanced Data Encoding Categorical Features Data Splitting Step 1 : Data Collection cornerstone machine learning rooted data . Collecting data involves gathering information aligned goals objectives AI project . feed subpar low - quality data model , produce satisfactory outcomes . holds true regardless model complexity , expertise data scientist , financial investment project [ 3 ] . companies accumulating data years , ensuring steady supply machine learning , lacking sufficient data turn reference datasets available online complete AI projects . Discovering new data sharing achieved methods : collaborative analysis ( DataHub ) , web ( Google Fusion Tables , CKAN , Quandl , Data Market ) , combination collaboration web use ( Kaggle ) . Additionally , specialized data retrieval systems , including data lakes ( Google Data Search ) web - based platforms ( WebTables ) [ 3 ] . Suppose necessary data ; proceed creating dataset . # import library import pandas pd import matplotlib.pyplot plt import seaborn sns # load data df = pd.read_csv('data / credit_scoring_eng.csv ' ) df.head(5 ) Result : Data Description children - number children family days_employed - number days employed dob_years - client age years education - client education level education_id - education identifier family_status - marital status family_status_id - marital status identifier gender - client gender income_type - type employment debt - client loan debt total_income - monthly income purpose - purpose loan application Step 2 : Data Cleaning involves identifying correcting errors inconsistencies data , missing values , outliers duplicates . techniques data cleaning , imputation , removal transformation [ 4 ] . Implementing phyton : 2 - : Handling missing values # check dataset information df.info ( ) Result : Findings : missing values columns days_employed total_income , number rows 21,525 # check percentage df.isna().sum ( ) / len(df ) Result : Findings : missing value percentage columns 10 % # Visualizing Missing Data seaborn heatmap . plt.figure(figsize=(10,6 ) ) sns.heatmap(df.isna().transpose ( ) , cmap="YlGnBu " , cbar_kws={'label ' : ' Missing Data ' } ) Result : Findings : 1 . Missing values form pattern . missing values caused job types clients job types ‘ student ’ ‘ unemployed ’ income , leading leave ‘ days_employed ’ ‘ total_income ’ columns . 2 . conclusion reinforced pattern shown seaborn heatmap , indicating value ‘ days_employed ’ column missing , data row ‘ total_income ’ missing ( symmetrical ) . 3 . missing values present ‘ days_employed ’ ‘ total_income ’ columns , columns float data types , fall Numeric / Ratio category , missing data filled statistical calculations ( Mean , Median ) . 4 . Median chosen fill missing values prevent occurrence outliers [ 5 ] #   function fill missing values median def data_imputation(data , column_grouping , column_selected ): # Parameter meaning # data = > dataframe processed # column_grouping = > column group values median # column_selected = > column fill NaN values # unique category groups group = data[column_grouping].unique ( ) # Loop value group category value group : # median median = data.loc[(data[column_grouping]==value ) & ~(data[column_selected].isna ( ) ) , column_selected].median ( ) # change missing value data.loc[(data[column_grouping]==value ) & ( data[column_selected].isna ( ) ) , column_selected ] = median # Return dataframe filling missing values return data # apply function ' total_income ' column df = data_imputation(data = df , column_grouping='age_category ' , column_selected='total_income ' ) # apply function ' days_employed ' column df = data_imputation(data = df , column_grouping='age_category ' , column_selected='days_employed ' ) # check statistical df.info ( ) Result : 2 - b : Handling outliers # check outlier children column sns.boxplot(df['children ' ] ) # check statistical data children column df['children'].describe ( ) Findings : 1 . Based statistical data , replace value 20 value 2 , assuming input error . 2 . remove minus sign ( - ) , assuming input error # replace value 20 value 2 condition_children = df['children']==20 df['children ' ] = df['children'].mask(condition_children , 2 ) # remove minus sign df['children ' ] = abs(df['children ' ] ) # check outliers days_employed column sns.boxplot(df['days_employed ' ] ) # check percentage len(df.loc[(df['days_employed ' ] < 0 ) | ( df['days_employed ' ] > 200000 ) ] ) / len(df ) Result : 0.8990011614401858 Findings : 2 issues identified ‘ days_employed ’ column : digits decimal point . Existence negative values outliers , high percentage rows having conditions , approximately 89 % . 2 . steps solve issues follows : Remove minus sign ( - ) . Perform rounding . Replace outlier values . # remove minus sign ( - ) , assuming input error df['days_employed ' ] = abs(df['days_employed ' ] ) # round df['days_employed ' ] = round(df['days_employed'],0 ) # check data distribution df['days_employed'].describe ( ) Result : Findings : mean value represent data mixed outliers . , replacement outliers median value . # Replace outlier median condition_de = ( df['days_employed ' ] > 200000 ) & ( df['days_employed'].notnull ( ) ) df['days_employed ' ] = df['days_employed'].mask(condition_de , df['days_employed'].median ( ) ) # verify result sns.boxplot(df['days_employed ' ] ) Result : 2 - c : Handling Duplicates Findings 1 . 72 identified duplicate data entries . 2 . duplicate data entries removed , index reset . # remove duplicate data reset index df = df.drop_duplicates().reset_index(drop = True ) visit GitHub account access complete code related example : GitHub … Contribute … github.com Step 3 : Data Transformation Data transformation involves technically converting data format , standard , structure , changing dataset content . typically prepare data consumption application user , enhance data quality . specifics data transformation vary based techniques employed . article , utilize data aggregation technique data transformation [ 6 ] . Data aggregation method present data summarized form . Given likelihood data originating diverse sources , combining incoming data cohesive description essence data aggregation . facet data processing holds significance hinges quality quantity data hand . illustrative example process generating annual sales report consolidating quarterly monthly data [ 7 ] . ways aggregate data Pandas , including : a. Utilizing groupby ( ) function : Grouping involves breaking dataset smaller subsets depending specific variables . approach widely employed data exploration analysis purposes . pandas ’ groupby ( ) function highly versatile allows grouping data based columns . groupby ( ) function , group data according selected variables subsequently apply range aggregation functions groups [ 8 ] . Implementing phyton : use groupby ( ) analyze user review professional review influence platform sales # preparing dataset top2_ref_df = reference_df.groupby(['platform ' , ' name'])[['total_sales ' , ' critic_score ' , ' user_score']].sum().query('platform = = " PS3 " & critic_score > 0 & user_score > 0').reset_index ( ) top2_ref_df = top2_ref_df[['name ' , ' platform ' , ' total_sales ' , ' critic_score ' , ' user_score ' ] ] top2_ref_df b. pivot_table ( ) function : explored GroupBy concept enables investigate connections dataset . pivot table similar operation commonly encountered spreadsheet software programs working tabular data . pivot table , input data column - wise format organized - dimensional table , offering multidimensional summary information . Distinguishing pivot tables GroupBy confusing times . helpful consider pivot tables essentially multidimensional form GroupBy aggregation . words , perform split - apply - combine process , case , splitting combining occur - dimensional index , - dimensional grid [ 9 ] . Implementing phyton : order analyze 5 platforms NA , EU JP regions visualize variation market share region # data aggregation sales platform NA , EU , JP regions agg_selected_region_platform = pd.pivot_table(data = reference_df , index='platform ' , values = [ ' na_sales ' , ' eu_sales ' , ' jp_sales ' ] , aggfunc = ' sum').sort_values(by='jp_sales ' ) agg_selected_region_platform visualize data : # visualizing sales platform NA , EU , JP regions plt.figure(figsize=(20,6 ) ) plt.title('Distribution game sales platform EU , NA , JP regions ' ) sns.lineplot(data = agg_selected_region_platform ) plt.show ( ) visit GitHub account access complete code related example : GitHub … Contribute … github.com Step 4 : Feature Engineering : Scaling , Normalization Standardization Feature engineering constitutes pivotal stage creation accurate efficient machine learning models . significant facet feature engineering involves scaling , normalization , standardization , encompassing alteration data enhance suitability modeling . Employing methods enhance model accuracy , mitigate influence outliers , ensure uniformity data scale . article delves fundamentals scaling , normalization , standardization [ 10 ] . Feature Scaling Feature scaling crucial step data preprocessing , aiming standardize values features variables dataset uniform scale . primary objective ensure features fair influence model , avoiding dominance features higher values . necessity feature scaling arises working datasets encompass features having diverse ranges , units measurement , orders magnitude . scenarios , discrepancies feature values introduce bias model performance hinder learning process . application feature scaling , features dataset harmonized consistent scale , simplifying construction precise efficient machine learning models . Scaling promotes meaningful feature comparisons , enhances model convergence , prevents specific features dominating solely based magnitude [ 10 ] . Use Feature Scaling ? Certain machine learning algorithms exhibit sensitivity feature scaling , remain unaffected . Let delve detailed examination aspect . Gradient Descent Based Algorithms Machine learning algorithms use gradient descent optimization technique ( like linear regression , logistic regression , etc ) require data scaled [ 10 ] 2 . Distance - Based Algorithms Algorithms based distance metrics , K - nearest neighbors ( KNN ) , K - means clustering , support vector machines ( SVM ) , highly influenced range features . algorithms rely calculating distances data points ascertain similarity [ 10 ] . Implementing Phyton : function created calculate distance k - nearest neighbors algorithm based distance metrics : Euclidean Manhattan . compare distance results unscaled scaled data . df dataset : # function calculating kNN distance def get_knn(df , n , k , metric ): " " " Display k nearest neighbors : param df : Pandas DataFrame find similar objects param n : number object k nearest neighbors sought param k : number k nearest neighbors displayed param metric : distance metric " " " nbrs = sklearn.neighbors . NearestNeighbors(n_neighbors = k , metric = metric , algorithm = ' brute ' ) nbrs.fit(df[feature_names ] ) nbrs_distances , nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names ] ] , k , return_distance = True ) df_res = pd.concat ( [ df.iloc[nbrs_indices[0 ] ] , pd . DataFrame(nbrs_distances . T , index = nbrs_indices[0 ] , columns=['distance ' ] ) ] , axis=1 ) return df_res unscaled data ( df ): # euclidean metric - unscaled data get_knn(df , 1 , 50 , ' euclidean ' ) Result : # manhattan metric - unscaled data get_knn(df , 1 , 50 , ' manhattan ' ) Findings : unscaled data , results ( referring generated indices — index 1 , similar classification following indices : 3920 , 4948 , 2528 , 3593 - ) distance metrics . scaled data : instance , age income different scales ( age = years , income = dollars ) , data scaling necessary . MaxAbsScaler utilized scale data maximum value ; , dividing observation maximum value variable : result previous transformation distribution values roughly vary range -1 1 . # scalling data MaxAbsScaler feature_names = [ ' gender ' , ' age ' , ' income ' , ' family_members ' ] transformer_mas = sklearn.preprocessing . MaxAbsScaler().fit(df[feature_names].to_numpy ( ) ) df_scaled = df.copy ( ) df_scaled.loc [: , feature_names ] = transformer_mas.transform(df[feature_names].to_numpy ( ) ) df_scaled dataset : # euclidean metric - scaled data get_knn(df_scaled , 1 , 10 , ' euclidean ' ) # manhattan metric - scaled data get_knn(df_scaled , 1 , 10 , ' manhattan ' ) question : non - scaled data affect kNN algorithm ? , affect ? Yes , data scaled , results ( regardless metric ) . , results inaccurate differences scales column . calculations , important maintain consistent scale possible . example : age income different scales ( age = years , income = dollars ) . visit GitHub account access complete code related example : GitHub … Contribute … github.com Normalization Normalization , data preprocessing approach , standardizes feature values dataset consistent scale . carried streamline data analysis modeling , mitigating influence disparate scales machine learning model accuracy [ 10 ] . Implementing Phyton : # import library sklearn.preprocessing import MinMaxScaler # fit scaler training data std = MinMaxScaler().fit(X_train ) # transform training data X_train_std = std.transform(X_train ) # transform testing data X_test_std = std.transform(X_test ) Standardization Standardization , form scaling , involves centering values mean adjusting standard deviation unit . Consequently , attribute mean zero , resulting distribution maintains unit standard deviation [ 10 ] . , let proceed utilizing scikit - learn StandardScaler standardizing features . process involves eliminating mean adjusting scale unit variance , ultimately resulting mean 0 standard deviation 1 . aligns data standard normal distribution . [ 11 ] . Implementing Phyton : compare metric results implementing StandardScaler . normalizing features : beforeScaling_lr = LogisticRegression(random_state = 42 ) # train model training set beforeScaling_lr.fit(features_train , target_train ) #   predict validation set y_predict_valid_lr = beforeScaling_lr.predict(features_valid ) # measuring probability validation set y_probability_valid_lr = beforeScaling_lr.predict_proba(features_valid ) [: , 1 ] # test performance algorithm F1 score auc_score print('F1 score = ' , f1_score(target_valid , y_predict_valid_lr ) ) print('AUC - ROC score = ' , roc_auc_score(target_valid , y_probability_valid_lr ) ) Result : Normalizing features : # normalizing features StandardScaler scaler = StandardScaler ( ) features_train[df_numerical ] = scaler.fit_transform(features_train[df_numerical ] ) features_valid[df_numerical ] = scaler.transform(features_valid[df_numerical ] ) features_test[df_numerical ] = scaler.transform(features_test[df_numerical ] ) normalizing features : afterScaling_lr = LogisticRegression(random_state = 42 ) # train model training set afterScaling_lr.fit(features_train , target_train ) #   predict validation set y_predict_valid_lr = afterScaling_lr.predict(features_valid ) # measuring probability validation set y_probability_valid_lr = afterScaling_lr.predict_proba(features_valid ) [: , 1 ] # test performance algorithm F1 score auc_score print('F1 score = ' , f1_score(target_valid , y_predict_valid_lr ) ) print('AUC - ROC score = ' , roc_auc_score(target_valid , y_probability_valid_lr ) ) Result : Findings : F1 score AUC - ROC score increasing standardizing features . visit GitHub account access complete code related example : GitHub … Contribute … github.com 2 , delve topics Feature Selection , handling imbalanced dataset , Encoding Features Data Splitting . eye continuation explore essential steps detail ! References : 1 . Shaomin Wu , review coarse warranty data analysis ( 2013 ) 2 . George Lawton , Data Preprocessing ( 2022 ) https://www.techtarget.com/searchdatamanagement/definition/data-preprocessing 3 . Yuliia Kniazieva , Data Collectio Machine Learning ( 2022 ) https://labelyourdata.com/articles/data-collection-methods-AI 4 . Deepak Jain , Data Preprocessing Data Mining ( 2023 ) https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/ 5 . https://stats.stackexchange.com/questions/143700/which-is-better-replacement-by-mean-and-replacement-by-median 6 . Chiradeep BasuMallick , Data Transformation ? Types , Tools , Importance ( 2022 ) https://www.spiceworks.com/tech/big-data/articles/what-is-data-transformation/ 7 . Data Science Wizards , Introduction Data Transformation ( 2023 ) Introduction Data Transformation End - - end data analysis involves processes practices aimed extracting insights data . data … medium.com 8 . https://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html 9 . https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html 10 . Aniruddha Bhandari , Feature Engineering : Scalling , Normalization Standardization ( 2023 ) Feature Engineering : Scaling , Normalization , Standardization ( Updated 2023 ) Learn feature scaling , normalization , & standardization work machine learning . Understand uses & differences … www.analyticsvidhya.com 11 . Scikit - learn documentation , Standard Scaler https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html 12 . Nate Rosidi , Advanced Feature Selection Techniques Machine Learning Models ( 2023 ) Advanced Feature Selection Techniques Machine Learning Models — KDnuggets Mastering Feature Selection : Exploration Advanced Techniques Supervised Unsupervised Machine Learning … www.kdnuggets.com Machine Learning Python Pandas Data Preprocessing Technology -- -- 4 Follow Published Women Technology 2.6 K Followers · published 2 hours ago Women Tech publication highlight women STEM , accomplishments , career lessons , stories . Follow Follow Written Learn Nas 263 Followers · 4 Following System Analyst | Data Engineering Machine Learning Enthusiast Follow Responses ( 4 ) responses Help Status Careers Press Blog Privacy Rules Terms Text speech