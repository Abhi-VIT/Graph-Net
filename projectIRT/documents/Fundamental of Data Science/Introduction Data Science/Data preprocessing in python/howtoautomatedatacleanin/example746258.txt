Automate Data Cleaning Preprocessing Python : Comprehensive Guide | Khushi Bhadange | Medium Open app Sign Sign Write Sign Sign Automate Data Cleaning Preprocessing Python : Comprehensive Guide Khushi Bhadange Follow 16 min read · Jan 1 , 2024 -- 1 Listen Share Photo Myriam Jessier Unsplash Key Tools Libraries Automation Python Pandas : Data manipulation analysis . Offers data structures like DataFrames Series easy data manipulation , handling missing data , merging , reshaping , slicing . Numpy : Numerical Computing Supports large , multi - dimensional arrays matrices , collection mathematical functions operate arrays . Scikit - learn : Machine learning data preprocessing . Provides tools data preprocessing , scaling , normalization , encoding categorical variables . offers imputation techniques handling missing values . SciPy : Scientific technical computing . advanced computations like optimization , linear algebra , integration , statistics . Matplotlib / Seaborn : Data visualization . Essential exploring data , identifying patterns , detecting outliers . Openpyxl / xlrd : Excel file manipulation . Read write Excel files , useful datasets Excel format . Beautiful Soup / Requests : Web scraping . Extract data HTML XML files , useful gathering data web . “ machine learning , garbage means garbage . automated data processing convenience , necessity . ” Starting Google Colab Google Colab blog . executed R Python . Python automated data preprocessing cleaning blog . Runtime > Change runtime choose necessary language wish modify runtime . Change Runtime Type — [ Image Author ] find complete code file end blog . data cleaning data preprocessing ? Preprocessing cleaning data seen essential phases data analysis machine learning , special function significance . , recommended perform data pretreatment cleaning analysis developing machine learning ( ML ) model . “ Data collection processing machine learning gathering information ; building foundation intelligent decision - making . ” Data Cleaning Data cleaning involves finding fixing ( eliminating ) corrupt inaccurate data datasets . Tasks Include : - Handling missing values . - Smoothing noisy data . - Finding removing outliers - Putting inconsistencies right Data Preprocessing Data preprocessing describes set operations carried clean data modify , arrange , extract meaningful insights . Tasks Include : - Data normalization ( Scaling data standard range ) . - Data transformation ( Converting data suitable structure ) . - Feature selection ( Selecting relevant variables analysis ) . - Data interpretation ( Data visualization visual analysis ) . Traditional vs Automated Data Cleaning Preprocessing Traditional Method - Manual Action : Relies primarily human labor find fix data issues . - Time - consuming : process laborious , particularly dealing big datasets . - Easily Mistaken Humans : Manual data cleansing result biases additional mistakes . Automated Method - Efficiency : Automated tools far faster data cleaning processing speed . - Scalability : appropriate managing complicated big datasets . - Consistency : Avoids human error ensures consistency data cleansing procedure . Let start execution . Adult Income Dataset ( Census Income ) Data cleaning Data preprocessing implementation . dataset mix categorial numerical data . help understand significance data cleaning preprocessing better . directly import dataset Python file . , need Install ucimlrepo package . # Installing ucimlrepo package pip install ucimlrepo # Importing dataset code ucimlrepo import fetch_ucirepo # fetch dataset adult = fetch_ucirepo(id=2 ) # data ( pandas dataframes ) X = adult.data.features y = adult.data.targets # metadata print(adult.metadata ) # variable information print(adult.variables ) [ Image 1 ] Output — Import dataset code view rows dataset understand dataset deeper . # Display rows dataset print(X.head ( ) ) [ Image 2 ] Output — Display rows dataset dataset mix categorial numerical values , check numerical features . # Summary statistics numerical features print(X.describe ( ) ) [ Image 3 ] Output — Summary numerical features Handling Missing Values check missing values . # Check missing values print(X.isnull().sum ( ) ) [ Image 4 ] Output — Check missing values , output , missing values categorical numerical data . ‘ isnull ( ) ’ function identify missing values represented ‘ NaN ’ ( Number ) numerical columns . detects missing values categorial string columns , represented ‘ NaN ’ ( Number)or ‘ ’ . want check missing values particular data type ( like numerical columns ) , need filter DataFrame data types apply ‘ isnull ( ) ’ . code missing values numerical features . # Assuming ' X ' DataFrame numerical_features = X.select_dtypes(include=['int64 ' , ' float64 ' ] ) # Count missing values numerical column missing_values_numerical = numerical_features.isnull().sum ( ) # Display missing values count numerical features print(missing_values_numerical ) [ Image 5 ] Output — Missing values Numerical features code missing values categorial features . # Assuming ' X ' DataFrame categorical_features = X.select_dtypes(include=['object ' ] ) # Count missing values categorical column missing_values_categorical = categorical_features.isnull().sum ( ) # Display missing values count categorical features print(missing_values_categorical ) [ Image 6 ] Output — Missing values categorial features check data type feature apply categorical numerical functions efficiently . # Check data types print(X.dtypes ) [ Image 7 ] Output — Check data types features missing values , decide way handle . common methods handle missing values imputation , dropping rows / columns . dataset , use Imputation method . imputation method , impute missing values median ( numerical features ) mode ( categorial features ) . , import SimpleImputer sklearn.impute # Impute missing values median ( numerical columns ) mode ( categorical columns ) sklearn.impute import SimpleImputer import numpy np # Imputer numerical data num_imputer = SimpleImputer(strategy='median ' ) num_cols = X.select_dtypes(include=['int64 ' , ' float64']).columns X[num_cols ] = num_imputer.fit_transform(X[num_cols ] ) # Imputer categorical data cat_imputer = SimpleImputer(strategy='most_frequent ' ) cat_cols = X.select_dtypes(include=['object']).columns X[cat_cols ] = cat_imputer.fit_transform(X[cat_cols ] ) [ Image 8 ] Output — Imputation Method Handling Missing Values Let understand exactly happens Imputation Method 1 . Numerical Features : handle missing values , replace missing values mean , mode , median column . Mean Imputation : imputation method , replace missing values mean column , suitable data outliers . Median Imputation : imputation method , replace missing values median column , suitable data outliers . Mode Imputation : imputation method suitable categorical data mode numerical data especially discrete . 2 . Categorical Features : handle missing values , replace missing values mode ( frequently occurring category ) . moving ensure missing values dataset . # Recheck missing values numerical column handling missing_values_numerical_after = numerical_features.isnull().sum ( ) # Display missing values count numerical features handling print("Missing Values Numerical Columns Handling:\n " , missing_values_numerical_after ) # Recheck missing values categorical column handling missing_values_categorical_after = categorical_features.isnull().sum ( ) # Display missing values count categorical features handling print("Missing Values Categorical Columns Handling:\n " , missing_values_categorical_after ) [ Image 9 ] Output — Rechecking missing values Numerical features [ Image 10 ] Output — Rechecking missing values Categorical features , missing values numerical categorical features implementing Imputation Method . Handling outliers moving forward look outliers dataset . starting implementation outliers , need understand things . Outliers typically identified numerical values represent quantitative values , allowing statistical analysis . hand , categorical features consist discrete labels categories , concept outliers based distance deviation applicable . Instead , categorical data infrequent categories outliers . , identify outliers numerical features . use Interquartile Range ( IQR ) identify outliers numerical features . need follow steps handling outliers follows : Extract column names numerical features : # Extracting column names numerical features numerical_feature_names = numerical_features.columns print(numerical_feature_names ) [ Image 11 ] Output — Extract column names numerical features 2 . Calculate IQR Identify Outliers : # Calculate IQR numerical column Q1 = X[numerical_feature_names].quantile(0.25 ) Q3 = X[numerical_feature_names].quantile(0.75 ) IQR = Q3 - Q1 # Determine outliers IQR outliers = ( ( X[numerical_feature_names ] < ( Q1 - 1.5 * IQR ) ) | ( X[numerical_feature_names ] > ( Q3 + 1.5 * IQR ) ) ) calculating IQR , filter DataFrame display rows outliers . # Filter DataFrame display rows outliers outlier_rows = X[outliers.any(axis=1 ) ] print(outlier_rows ) display outliers numerical feature individually . # Display outliers numerical column individually col numerical_feature_names : outliers[col].any ( ): print(f"Outliers { col } : " ) print(X.loc[outliers[col ] , col ] ) print("\n " ) Output : Outliers age : 74        79.0 222       90.0 430       80.0 918       81.0 1040      90.0 ... 48524     80.0 48564     80.0 48667     83.0 48709     79.0 48812     81.0 : age , Length : 216 , dtype : float64 Outliers fnlwgt : 37        544091.0 40        507875.0 80        446839.0 110       432376.0 157       494223.0 ... 48677     427515.0 48734     607658.0 48740     422933.0 48821     430340.0 48833     440129.0 : fnlwgt , Length : 1453 , dtype : float64 Outliers education - num : 15        4.0 56        3.0 61        4.0 79        4.0 160       2.0 ... 48638     4.0 48645     4.0 48766     4.0 48782     4.0 48791     4.0 : education - num , Length : 1794 , dtype : float64 Outliers capital - gain : 0          2174.0 8         14084.0 9          5178.0 59         5013.0 60         2407.0 ... 48805      5178.0 48812      2936.0 48813      7688.0 48822     15024.0 48840      5455.0 : capital - gain , Length : 4035 , dtype : float64 Outliers capital - loss : 23        2042.0 32        1408.0 52        1902.0 93        1573.0 96        1902.0 ... 48783     1602.0 48794     2057.0 48802     1590.0 48814     1669.0 48816     1902.0 : capital - loss , Length : 2282 , dtype : float64 Outliers hours - - week : 1         13.0 6         16.0 10        80.0 12        30.0 20        60.0 ... 48820      8.0 48822     55.0 48825     32.0 48827     32.0 48841     60.0 : hours - - week , Length : 13496 , dtype : float64 3 . Visualize outliers better understanding : import required libraries . import matplotlib.pyplot plt import seaborn sns Box Plot # Assuming ' X ' DataFrame ' numerical_feature_names ' numerical columns col numerical_feature_names : plt.figure(figsize=(10 , 6 ) ) sns.boxplot(x = X[col ] ) plt.title(f'Box Plot { col } ' ) plt.show ( ) view output code . Scatter Plot # Create scatter plots pairwise feature comparison # Note : computationally intensive large number features itertools import combinations pairs = combinations(numerical_feature_names , 2 ) ( col1 , col2 ) pairs : plt.figure(figsize=(10 , 6 ) ) sns.scatterplot(x = X[col1 ] , y = X[col2 ] ) plt.title(f'Scatter Plot { col1 } vs { col2 } ' ) plt.xlabel(col1 ) plt.ylabel(col2 ) plt.show ( ) giving scatter plot features . want scatter plot numerical features outliers lie , . analyzing graphs graphs provided link , easily look outliers . 4 . Handle outliers : methods handle outliers . removal outliers , cap / floor , transform method , etc . Removal outliers simplest method handling outliers . , method lead loss information removes outliers dataset . Use method outliers represent errors relevant analysis . # Remove rows outliers X_cleaned = X[~(outliers.any(axis=1 ) ) ] Cap / Floor Outliers ( Winsorizing ) method , replace outliers nearest boundary values . preserve shape data distribution reducing impact outliers . # Cap floor outliers X_cleaned = X.copy ( )   # Create copy original DataFrame preserve original data col numerical_feature_names : lower_bound = Q1[col ] - 1.5 * IQR[col ] upper_bound = Q3[col ] + 1.5 * IQR[col ] X_cleaned[col ] = X_cleaned[col].clip(lower = lower_bound , upper = upper_bound ) Transforming Data Apply transformations like log , square root reduce effect outliers . useful data skewed distribution . # Apply log transformation import numpy np X_cleaned = X.copy ( )   # Create copy original DataFrame transformation col numerical_feature_names : # Apply log transformation np.log1p log(x + 1 ) # helps avoiding issues zero negative values data X_cleaned[col ] = np.log1p(X_cleaned[col ] ) Note : code file provided end article , gave codes methods . choose method want apply . Rerun code cells given rechecking outliers select commented methods . 5 . Recheck outliers : # Verify checking statistical summary plotting print(X_cleaned.describe ( ) ) # use plots like boxplots , histograms , etc . [ Image 12 ] Output — Verify Outliers # Assuming ' X_cleaned ' DataFrame outlier handling # ' numerical_feature_names ' numerical column names itertools import combinations # Generate scatter plots pair numerical features ( col1 , col2 ) combinations(numerical_feature_names , 2 ): plt.figure(figsize=(10 , 6 ) ) sns.scatterplot(x = X_cleaned[col1 ] , y = X_cleaned[col2 ] ) plt.title(f'Scatter Plot { col1 } vs { col2 } Outlier Handling ' ) plt.xlabel(col1 ) plt.ylabel(col2 ) plt.show ( ) want view output code , click . outliers handled . Encoding Categorical Values encoding categorical values necessary ? machine learning , algorithms require numerical data . dataset numerical categorical data categorical data converted numerical format , called encoding . encoding methods methods Hot encoding Ordinal encoding . - Hot encoding method , create new columns indicating presence possible value original data . suitable nominal data ordinal relationship exists . encoding method increase number features dataset dataset categorical levels . called curse dimensionality reduced techniques like feature selection . Ordinal encoding method converts category number . suitable ordinal data . Data said ordinal categories meaningful order . dataset nominal values , - Hot encoding encoding categorical values . , identify categorical columns . # Identify categorical columns # Assuming ' X ' DataFrame # Select categorical columns categorical_features = X.select_dtypes(include=['object ' ] ) # names categorical columns categorical_feature_names = categorical_features.columns # Display names categorical columns print("Categorical Columns : " ) print(categorical_feature_names ) [ Image 13 ] Output — Extracting Categorical Columns apply - Hot Encoding . # - Hot Encoding categorical features import pandas pd X_encoded = pd.get_dummies(X[categorical_feature_names ] ) # Combine encoded categorical features cleaned numerical features X_final = pd.concat([X_cleaned , X_encoded ] , axis=1 ) good practice check DataFrame change . # Verify structure dataframe encoding print(X_final.head ( ) ) print(X_final.info ( ) ) Output : age          workclass     fnlwgt   education   education - num   \ 0   39.0          State - gov    77516.0   Bachelors            13.0 1   50.0   Self - emp - - inc    83311.0   Bachelors            13.0 2   38.0            Private   215646.0     HS - grad             9.0 3   53.0            Private   234721.0        11th             7.0 4   28.0            Private   338409.0   Bachelors            13.0 marital - status          occupation    relationship    race      sex   ...   \ 0        - married        Adm - clerical   - - family   White     Male   ... 1   Married - civ - spouse     Exec - managerial         Husband   White     Male   ... 2             Divorced   Handlers - cleaners   - - family   White     Male   ... 3   Married - civ - spouse   Handlers - cleaners         Husband   Black     Male   ... 4   Married - civ - spouse      Prof - specialty            Wife   Black   Female   ... native - country_Portugal   native - country_Puerto - Rico   \ 0                         0                            0 1                         0                            0 2                         0                            0 3                         0                            0 4                         0                            0 native - country_Scotland native - country_South   native - country_Taiwan   \ 0                         0                     0                       0 1                         0                     0                       0 2                         0                     0                       0 3                         0                     0                       0 4                         0                     0                       0 native - country_Thailand   native - country_Trinadad&Tobago   \ 0                         0                                0 1                         0                                0 2                         0                                0 3                         0                                0 4                         0                                0 native - country_United - States   native - country_Vietnam   \ 0                              1                        0 1                              1                        0 2                              1                        0 3                              1                        0 4                              0                        0 native - country_Yugoslavia 0                           0 1                           0 2                           0 3                           0 4                           0 [ 5 rows x 116 columns ] < class ' pandas.core.frame . DataFrame ' > RangeIndex : 48842 entries , 0 48841 Columns : 116 entries , age native - country_Yugoslavia dtypes : float64(6 ) , object(8 ) , uint8(102 ) memory usage : 10.0 + MB , DataFrame size increased , memory usage . transformed DataFrame suitable input machine learning algorithms input numerical data . cases , especially linear models , advisable drop binary columns original categorical variable avoid multicollinearity ( known dummy variable trap ) . Feature Scaling feature scaling ? Brings features scale , ensuring feature dominates model scale . essential algorithms calculate distance data points like KNN algorithm . Helps speeding convergence case Gradient descent algorithm . methods feature scaling follows : Standardization ( Z score normalization ) method transforms feature Mean 0 Standard Deviation 1 . Min- Max scaling method scales features fixed range usually 0 1 . dataset article , Standardization method . Feature Scaling check non - numeric data type . # Checks non numeric data type print(X_encoded.dtypes ) # Split dataset sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split(X_encoded , y , test_size=0.2 , random_state=42 ) # Apply feature Scaling scaler = StandardScaler ( ) # Fit scaler training data transform training data X_train_scaled = scaler.fit_transform(X_train ) # Transform test data scaler X_test_scaled = scaler.transform(X_test ) # Convert scaled data dataframe X_train_scaled = pd . DataFrame(X_train_scaled , columns = X_train.columns ) X_test_scaled = pd . DataFrame(X_test_scaled , columns = X_test.columns ) # Check statistical summary scaled training data print(X_train_scaled.describe ( ) ) check output code . Handling Imbalanced Data Handling imbalance crucial step machine learning models , especially classification problems distribution classes skewed . handling imbalance , assess extent imbalance target variable y. # Check distribution target variable class_counts = y_train.value_counts ( ) print(class_counts ) [ Image 14 ] Output — Distribution Target variable idea imbalanced dataset . techniques handle imbalanced data . selection technique depends extent imbalanced data . Oversampling Minority Class : involves adding copies minority class . Techniques like SMOTE ( Synthetic Minority - sampling Technique ) generate synthetic samples creating exact copies . Undersampling Majority Class : involves reducing number instances majority class balance dataset . Class Weights : machine learning algorithms allow adjust weights inversely proportional class frequencies . SMOTE technique . # Handling imbalanced data imblearn.over_sampling import SMOTE smote = SMOTE(random_state=42 ) # Apply SMOTE training data X_train_balanced , y_train_balanced = smote.fit_resample(X_train_scaled , y_train ) # Assuming y_train_balanced target variable applying SMOTE balancing technique # Check new class distribution directly new_class_counts = y_train_balanced.value_counts ( ) print(new_class_counts ) [ Image 15 ] Output — Handling Imbalanced Data Feature Selection Feature selection important step machine learning model . reduces overfitting training time . , increases accuracy . implementing feature selection feature importance . use machine learning algorithms like Decision Trees Random Forests rank importance features . understand feature selection , create model features model selected features . perform comparative analysis models . # Train model selected features sklearn.ensemble import RandomForestClassifier sklearn.metrics import classification_report , accuracy_score # Create model model = RandomForestClassifier(n_estimators=100 , random_state=42 ) # Train model selected features training set model.fit(X_train_selected , y_train_balanced ) # predictions y_pred = model.predict(X_test_selected ) # Evaluate model accuracy = accuracy_score(y_test , y_pred ) print("Accuracy : " , accuracy ) print("\nClassification Report:\n " , classification_report(y_test , y_pred ) ) [ Image 16 ] Output — Accuracy report selected feature # Train model feature set comparison model_full = RandomForestClassifier(n_estimators=100 , random_state=42 ) model_full.fit(X_train_balanced , y_train_balanced ) # predictions y_pred_full = model_full.predict(X_test_scaled ) # Evaluate model accuracy_full = accuracy_score(y_test , y_pred_full ) print("Accuracy features : " , accuracy_full ) print("\nClassification Report features:\n " , classification_report(y_test , y_pred_full ) ) [ Image 17 ] Output — Accuracy report features increase accuracy model selected features use hyperparameter tuning . way automated data cleaning data preprocessing actually building machine learning model . Building Preprocessing pipelines create pipeline includes scaling , encoding , preprocessing steps . , example building preprocessing pipeline . sklearn.pipeline import Pipeline sklearn.impute import SimpleImputer sklearn.compose import ColumnTransformer numeric_transformer = Pipeline(steps= [ ( ' imputer ' , SimpleImputer(strategy='mean ' ) ) , ( ' scaler ' , StandardScaler ( ) ) ] ) categorical_transformer = Pipeline(steps= [ ( ' imputer ' , SimpleImputer(strategy='constant ' , fill_value='missing ' ) ) , ( ' onehot ' , OneHotEncoder(handle_unknown='ignore ' ) ) ] ) preprocessor = ColumnTransformer ( transformers= [ ( ' num ' , numeric_transformer , numeric_features ) , ( ' cat ' , categorical_transformer , categorical_features ) ] ) clf = Pipeline(steps=[('preprocessor ' , preprocessor ) , ( ' classifier ' , SomeClassifier ( ) ) ] ) Creating pipeline reduce repetitive tasks model building faster accurate . , creating Pipeline dataset . # Creating Pipeline # Numeric Transformer numeric_transformer = Pipeline(steps= [ ( ' imputer ' , SimpleImputer(strategy='mean ' ) ) ,   # Imputes missing values mean ( ' scaler ' , StandardScaler ( ) )   # Standardizes data ] ) # Categorical Pipeline categorical_transformer = Pipeline(steps= [ ( ' imputer ' , SimpleImputer(strategy='constant ' , fill_value='missing ' ) ) ,   # Handles missing values ( ' onehot ' , OneHotEncoder(handle_unknown='ignore ' ) )   # - hot encodes categorical variables ] ) # Column Transformer preprocessor = ColumnTransformer ( transformers= [ ( ' num ' , numeric_transformer , numeric_features ) , ( ' cat ' , categorical_transformer , categorical_features ) ] ) # Create pipeline classifier clf = Pipeline(steps=[('preprocessor ' , preprocessor ) , ( ' classifier ' , RandomForestClassifier ( ) ) ] ) # Train model clf.fit(X_train , y_train ) [ Image 18 ] Output — Create Pipeline # Train model clf.fit(X_train , y_train ) [ Image 19 ] Output — Evaluate model Key Points Data cleaning data preprocessing crucial points data analysis pipeline . key points remember . Identification removal missing values . Outlier detection treatment . Data standardization normalization . Encoding categorical data . Handling imbalanced data . Feature engineering . Feature selection . Data validation . Automating process . Google Colab Notebook Article Python Python Notebook socials LinkedIn Github Machine Learning Data Science Data Analysis Automation Data Visualization -- -- 1 Follow Written Khushi Bhadange 5 Followers · 6 Following Passionate Computer Science student researcher focus Data Science . Dedicated unraveling complexities transforming clarity . Follow Responses ( 1 ) responses Help Status Careers Press Blog Privacy Rules Terms Text speech