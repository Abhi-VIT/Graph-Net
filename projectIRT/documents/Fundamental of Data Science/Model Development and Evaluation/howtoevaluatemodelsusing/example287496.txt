Evaluate Models MLflow - Databricks Blog Skip main content Login Databricks Discover Executives Startups Lakehouse Architecture Mosaic Research Customers Featured Stories Customers Partners Cloud Providers Databricks AWS , Azure , GCP Consulting & System Integrators Experts build , deploy migrate Databricks Technology Partners Connect existing tools Lakehouse C&SI Partner Program Build , deploy migrate Lakehouse Data Partners Access ecosystem data consumers Partner Solutions Find custom industry migration solutions Built Databricks Build , market grow business Product Databricks Platform Platform Overview unified platform data , analytics AI Data Management Data reliability , security performance Sharing open , secure , zero - copy sharing data Data Warehousing Serverless data warehouse SQL analytics Governance Unified governance data , analytics AI assets Real - Time Analytics Real - time analytics , AI applications simple Artificial Intelligence Build deploy ML GenAI applications Data Engineering ETL orchestration batch streaming data Business Intelligence Intelligent analytics real - world data Data Science Collaborative data science scale Integrations Data Marketplace Open marketplace data , analytics AI IDE Integrations Build Lakehouse favorite IDE Partner Connect Discover integrate Databricks ecosystem Pricing Databricks Pricing Explore product pricing , DBUs Cost Calculator Estimate compute costs cloud Open Source Open Source Technologies Learn innovations platform Solutions Databricks Industries Communications Media Entertainment Financial Services Public Sector Healthcare & Life Sciences Retail Manufacturing Industries Cross Industry Solutions Customer Data Platform Cybersecurity Migration & Deployment Data Migration Professional Services Solution Accelerators Explore Accelerators faster outcomes matter Resources Training Certification Learning Overview Hub training , certification , events Training Overview Discover curriculum tailored needs Databricks Academy Sign Databricks learning platform Certification Gain recognition differentiation University Alliance Want teach Databricks ? . Events Data + AI Summit Data + AI World Tour Data Intelligence Days Event Calendar Blog Podcasts Databricks Blog Explore news , product announcements , Databricks Mosaic Research Blog Discover latest Gen AI research Data Brew Podcast Let talk data ! Champions Data + AI Podcast Insights data leaders powering innovation Help Customer Support Documentation Community Dive Deep Resource Center Demo Center Company Team Databricks Ventures Contact Careers Working Databricks Open Jobs Press Awards Recognition Newsroom Security Trust Security Trust DATA + AI SUMMIT JUNE 9–12 | SAN FRANCISCO biggest Summit . Early - bird savings $ 600 — ends April 30 . REGISTER Ready started ? Demo DATA + AI SUMMIT JUNE 9–12 | SAN FRANCISCO biggest Summit . Early - bird savings $ 600 — ends April 30 . REGISTER Login Try Databricks Blog / Data Science ML / Article Model Evaluation MLflow Published : April 19 , 2022 Data Science ML 8 min read Mark Zhang Share post Subscribe data scientists ML engineers today use MLflow manage models . MLflow open - source platform enables users govern aspects ML lifecycle , including limited experimentation , reproducibility , deployment , model registry . critical step development ML models evaluation performance novel datasets . Motivation Evaluate Models ? Model evaluation integral ML lifecycle . enables data scientists measure , interpret , explain performance models . accelerates model development timeframe providing insights models performing way performing . Especially complexity ML models increases , able swiftly observe understand performance ML models essential successful ML development journey . State Model Evaluation MLflow Currently , users evaluate performance MLflow model python_function ( pyfunc ) model flavor mlflow.evaluate API , supports evaluation classification regression models . computes logs set built - task - specific performance metrics , model performance plots , model explanations MLflow Tracking server . evaluate MLflow models custom metrics included built - evaluation metric set , users define custom model evaluator plugin . involve creating custom evaluator class implements ModelEvaluator interface , registering evaluator entry point MLflow plugin . rigidity complexity prohibitive users . According internal customer survey , 75 % respondents frequently use specialized , business - focused metrics addition basic ones like accuracy loss . Data scientists utilize custom metrics descriptive business objectives ( e.g. conversion rate ) , contain additional heuristics captured model prediction . blog , introduce easy convenient way evaluating MLflow models user - defined custom metrics . functionality , data scientist easily incorporate logic model evaluation stage quickly determine best - performing model downstream analysis . * Note : MLflow 2.4 , mlflow.evaluate expanded support LLM text , text summarization , question answering models Usage Built - Metrics MLflow bakes set commonly performance model explainability metrics classifier regressor models . Evaluating models metrics straightforward . need create evaluation dataset containing test data targets mlflow.evaluate . Depending type model , different metrics computed . Refer Default Evaluator behavior section API documentation mlflow.evaluate - - date information built - metrics . Example simple example classifier MLflow model evaluated built - metrics . , import necessary libraries , split dataset , fit model , create evaluation dataset Finally , start MLflow run mlflow.evaluate find logged metrics artifacts MLflow UI : Custom Metrics evaluate model custom metrics , simply pass list custom metric functions mlflow.evaluate API . Function Definition Requirements Custom metric functions accept required parameters optional parameter following order : eval_df : Pandas Spark DataFrame containing prediction target column . E.g. output model vector numbers , eval_df DataFrame look like : builtin_metrics : dictionary containing built - metrics E.g. regressor model , builtin_metrics look like : ( Optional ) artifacts_dir : path temporary directory custom metric function temporarily store produced artifacts logging MLflow . E.g. Note look different depending specific environment setup . example , MacOS look like : file artifacts stored artifacts_dir , ensure persist complete execution mlflow.evaluate . Return Value Requirements function return dictionary representing produced metrics optionally return second dictionary representing produced artifacts . dictionaries , key entry represents corresponding metric artifact . metric scalar , ways define artifacts : path artifact file string representation JSON object pandas DataFrame numpy array matplotlib figure objects attempted pickled default protocol Refer documentation mlflow.evaluate - depth definition details . Example Let walk concrete example uses custom metrics . , create toy model California Housing dataset . , setup dataset model comes exciting : defining custom metrics function , custom artifact ! ! Finally , tie , start MLflow run mlflow.evaluate : Logged custom metrics artifacts found alongside default metrics artifacts . red boxed regions logged custom metrics artifacts run page . Accessing Evaluation Results Programmatically far , explored evaluation results built - custom metrics MLflow UI . , access programmatically EvaluationResult object returned mlflow.evaluate . Let continue custom metrics example access evaluation results programmatically . ( Assuming result EvaluationResult instance ) . access set computed metrics result.metrics dictionary containing scalar values metrics . content result.metrics look like : Similarly , set artifacts accessible result.artifacts dictionary . values entry EvaluationArtifact object . result.artifacts look like : Example Notebooks Short Example Comprehensive Example Underneath Hood diagram illustrates works hood : Conclusion blog post , covered : significance model evaluation currently supported MLflow . having easy way MLflow users incorporate custom metrics MLflow models important . evaluate models default metrics . evaluate models custom metrics . MLflow handles model evaluation scenes . Subscribe Share post miss Databricks post Subscribe categories care latest posts delivered inbox Sign ? Generative AI September 9 , 2024 / 5 min read Announcing Advanced Security Governance Mosaic AI Gateway Data Science ML October 1 , 2024 / 10 min read ICE / NYSE : Unlocking Financial Insights Custom Text - - SQL Application Databricks Discover Executives Startups Lakehouse Architecture Mosaic Research Customers Featured Partners Cloud Providers Technology Partners Data Partners Built Databricks Consulting & System Integrators C&SI Partner Program Partner Solutions Databricks Discover Executives Startups Lakehouse Architecture Mosaic Research Customers Featured Partners Cloud Providers Technology Partners Data Partners Built Databricks Consulting & System Integrators C&SI Partner Program Partner Solutions Product Databricks Platform Platform Overview Sharing Governance Artificial Intelligence Business Intelligence Data Management Data Warehousing Real - Time Analytics Data Engineering Data Science Pricing Pricing Overview Pricing Calculator Open Source Integrations Data Marketplace IDE Integrations Partner Connect Product Databricks Platform Platform Overview Sharing Governance Artificial Intelligence Business Intelligence Data Management Data Warehousing Real - Time Analytics Data Engineering Data Science Pricing Pricing Overview Pricing Calculator Open Source Integrations Data Marketplace IDE Integrations Partner Connect Solutions Databricks Industries Communications Financial Services Healthcare Life Sciences Manufacturing Media Entertainment Public Sector Retail View Cross Industry Solutions Customer Data Platform Cybersecurity Data Migration Professional Services Solution Accelerators Solutions Databricks Industries Communications Financial Services Healthcare Life Sciences Manufacturing Media Entertainment Public Sector Retail View Cross Industry Solutions Customer Data Platform Cybersecurity Data Migration Professional Services Solution Accelerators Resources Documentation Customer Support Community Training Certification Learning Overview Training Overview Certification University Alliance Databricks Academy Login Events Data + AI Summit Data + AI World Tour Data Intelligence Days Calendar Blog Podcasts Databricks Blog Databricks Mosaic Research Blog Data Brew Podcast Champions Data & AI Podcast Resources Documentation Customer Support Community Training Certification Learning Overview Training Overview Certification University Alliance Databricks Academy Login Events Data + AI Summit Data + AI World Tour Data Intelligence Days Calendar Blog Podcasts Databricks Blog Databricks Mosaic Research Blog Data Brew Podcast Champions Data & AI Podcast Company Team Databricks Ventures Contact Careers Open Jobs Working Databricks Press Awards Recognition Newsroom Security Trust Company Team Databricks Ventures Contact Careers Open Jobs Working Databricks Press Awards Recognition Newsroom Security Trust Databricks Inc. 160 Spear Street , 15th Floor San Francisco , 94105 1 - 866 - 330 - 0121 Careers Databricks © Databricks 2025 . rights reserved . Apache , Apache Spark , Spark , Spark Logo , Apache Iceberg , Iceberg , Apache Iceberg logo trademarks Apache Software Foundation . Privacy Notice | Terms Use | Modern Slavery Statement | California Privacy | Privacy Choices