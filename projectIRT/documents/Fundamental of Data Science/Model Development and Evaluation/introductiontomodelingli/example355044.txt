Python Data Analysis , 3E - 12    Introduction Modeling Libraries Python Python Data Analysis , 3E Wes Blog Data Notebooks ( GitHub ) Data Notebooks ( Gitee ) Chapters 12 Introduction Modeling Libraries Python Open Edition Chapters Preface 1 Preliminaries 2 Python Language Basics , IPython , Jupyter Notebooks 3 Built - Data Structures , Functions , Files 4 NumPy Basics : Arrays Vectorized Computation 5 Getting Started pandas 6 Data Loading , Storage , File Formats 7 Data Cleaning Preparation 8 Data Wrangling : Join , Combine , Reshape 9 Plotting Visualization 10 Data Aggregation Group Operations 11 Time Series 12 Introduction Modeling Libraries Python 13 Data Analysis Examples Appendices Advanced NumPy B IPython System Table contents 12.1 Interfacing pandas Model Code 12.2 Creating Model Descriptions Patsy Data Transformations Patsy Formulas Categorical Data Patsy 12.3 Introduction statsmodels Estimating Linear Models Estimating Time Series Processes 12.4 Introduction scikit - learn 12.5 Conclusion 12 Introduction Modeling Libraries Python Open Access web version Python Data Analysis 3rd Edition available companion print digital editions . encounter errata , report . note aspects site produced Quarto differ formatting print eBook versions O’Reilly . find online edition book useful , consider ordering paper copy DRM - free eBook support author . content website copied reproduced . code examples MIT licensed found GitHub Gitee . book , focused providing programming foundation data analysis Python . data analysts scientists report spending disproportionate time data wrangling preparation , book structure reflects importance mastering techniques . library use developing models depend application . statistical problems solved simpler techniques like ordinary squares regression , problems advanced machine learning methods . Fortunately , Python languages choice implementing analytical methods , tools explore completing book . chapter , review features pandas helpful crossing forth data wrangling pandas model fitting scoring . short introductions popular modeling toolkits , statsmodels scikit - learn . projects large warrant dedicated book , effort comprehensive instead direct projects ' online documentation Python - based books data science , statistics , machine learning . 12.1 Interfacing pandas Model Code common workflow model development use pandas data loading cleaning switching modeling library build model . important model development process called feature engineering machine learning . describe data transformation analytics extract information raw dataset useful modeling context . data aggregation GroupBy tools explored book feature engineering context . details " good " feature engineering scope book , methods switching data manipulation pandas modeling painless possible . point contact pandas analysis libraries usually NumPy arrays . turn DataFrame NumPy array , use to_numpy method : [ 12 ] : data = pd . DataFrame ( { .... : ' x0 ' : [ 1 , 2 , 3 , 4 , 5 ] , .... : ' x1 ' : [ 0.01 , - 0.01 , 0.25 , - 4.1 , 0 . ] , .... : ' y ' : [ - 1.5 , 0 . , 3.6 , 1.3 , - 2 . ] } ) [ 13 ] : data [ 13 ] : x0     x1     y 0 1 0.01 - 1.5 1 2 - 0.01 0.0 2 3 0.25 3.6 3 4 - 4.10 1.3 4 5 0.00 - 2.0 [ 14 ] : data.columns [ 14 ] : Index ( [ ' x0 ' , ' x1 ' , ' y ' ] , dtype = ' object ' ) [ 15 ] : data.to_numpy ( ) [ 15 ] : array ( [ [ 1 . , 0.01 , - 1.5 ] , [ 2 . , - 0.01 , 0 . ] , [ 3 . , 0.25 , 3.6 ] , [ 4 . , - 4.1 , 1.3 ] , [ 5 . , 0 . , - 2 . ] ] ) convert DataFrame , recall earlier chapters , pass - dimensional ndarray optional column names : [ 16 ] : df2 = pd . DataFrame(data.to_numpy ( ) , columns = [ ' ' , ' ' , ' ' ] ) [ 17 ] : df2 [ 17 ] :      0 1.0 0.01 - 1.5 1 2.0 - 0.01 0.0 2 3.0 0.25 3.6 3 4.0 - 4.10 1.3 4 5.0 0.00 - 2.0 to_numpy method intended data homogeneous — example , numeric types . heterogeneous data , result ndarray Python objects : [ 18 ] : df3 = data.copy ( ) [ 19 ] : df3 [ ' strings ' ] = [ ' ' , ' b ' , ' c ' , ' , ' e ' ] [ 20 ] : df3 [ 20 ] : x0     x1     y strings 0 1 0.01 - 1.5 1 2 - 0.01 0.0 b 2 3 0.25 3.6 c 3 4 - 4.10 1.3 d 4 5 0.00 - 2.0 e [ 21 ] : df3.to_numpy ( ) [ 21 ] : array ( [ [ 1 , 0.01 , - 1.5 , ' ' ] , [ 2 , - 0.01 , 0.0 , ' b ' ] , [ 3 , 0.25 , 3.6 , ' c ' ] , [ 4 , - 4.1 , 1.3 , ' ] , [ 5 , 0.0 , - 2.0 , ' e ' ] ] , dtype = object ) models , wish use subset columns . recommend loc indexing to_numpy : [ 22 ] : model_cols = [ ' x0 ' , ' x1 ' ] [ 23 ] : data.loc [: , model_cols].to_numpy ( ) [ 23 ] : array ( [ [ 1 . , 0.01 ] , [ 2 . , - 0.01 ] , [ 3 . , 0.25 ] , [ 4 . , - 4.1 ] , [ 5 . , 0 . ] ] ) libraries native support pandas work automatically : converting NumPy DataFrame attaching model parameter names columns output tables Series . cases , perform " metadata management " manually . Ch 7.5 : Categorical Data , looked pandas Categorical type pandas.get_dummies function . Suppose nonnumeric column example dataset : [ 24 ] : data [ ' category ' ] = pd . Categorical ( [ ' ' , ' b ' , ' ' , ' ' , ' b ' ] , .... :                                    categories = [ ' ' , ' b ' ] ) [ 25 ] : data [ 25 ] : x0     x1     y category 0 1 0.01 - 1.5 1 2 - 0.01 0.0 b 2 3 0.25 3.6 3 4 - 4.10 1.3 4 5 0.00 - 2.0 b wanted replace ' category ' column dummy variables , create dummy variables , drop ' category ' column , join result : [ 26 ] : dummies = pd.get_dummies(data.category , prefix = ' category ' , .... :                           dtype = float ) [ 27 ] : data_with_dummies = data.drop ( ' category ' , axis = 1 ) .join(dummies ) [ 28 ] : data_with_dummies [ 28 ] : x0     x1     y   category_a   category_b 0 1 0.01 - 1.5 1.0 0.0 1 2 - 0.01 0.0 0.0 1.0 2 3 0.25 3.6 1.0 0.0 3 4 - 4.10 1.3 1.0 0.0 4 5 0.00 - 2.0 0.0 1.0 nuances fitting certain statistical models dummy variables . simpler error - prone use Patsy ( subject section ) simple numeric columns . 12.2 Creating Model Descriptions Patsy Patsy Python library describing statistical models ( especially linear models ) string - based " formula syntax , " inspired ( exactly ) formula syntax R S statistical programming languages . installed automatically install statsmodels : conda install statsmodels Patsy supported specifying linear models statsmodels , focus main features help running . Patsy formulas special string syntax looks like : y ~ x0 + x1 syntax + b mean add b , terms design matrix created model . patsy.dmatrices function takes formula string dataset ( DataFrame dictionary arrays ) produces design matrices linear model : [ 29 ] : data = pd . DataFrame ( { .... : ' x0 ' : [ 1 , 2 , 3 , 4 , 5 ] , .... : ' x1 ' : [ 0.01 , - 0.01 , 0.25 , - 4.1 , 0 . ] , .... : ' y ' : [ - 1.5 , 0 . , 3.6 , 1.3 , - 2 . ] } ) [ 30 ] : data [ 30 ] : x0     x1     y 0 1 0.01 - 1.5 1 2 - 0.01 0.0 2 3 0.25 3.6 3 4 - 4.10 1.3 4 5 0.00 - 2.0 [ 31 ] : import patsy [ 32 ] : y , X = patsy.dmatrices ( ' y ~ x0 + x1 ' , data ) : [ 33 ] : y [ 33 ] : DesignMatrix shape ( 5 , 1 ) y - 1.5 0.0 3.6 1.3 - 2.0 Terms : ' y ' ( column 0 ) [ 34 ] : X [ 34 ] : DesignMatrix shape ( 5 , 3 ) Intercept   x0      x1 1 1 0.01 1 2 - 0.01 1 3 0.25 1 4 - 4.10 1 5 0.00 Terms : ' Intercept ' ( column 0 ) ' x0 ' ( column 1 ) ' x1 ' ( column 2 ) Patsy DesignMatrix instances NumPy ndarrays additional metadata : [ 35 ] : np.asarray(y ) [ 35 ] : array ( [ [ - 1.5 ] , [ 0 . ] , [ 3.6 ] , [ 1.3 ] , [ - 2 . ] ] ) [ 36 ] : np.asarray(X ) [ 36 ] : array ( [ [ 1 . , 1 . , 0.01 ] , [ 1 . , 2 . , - 0.01 ] , [ 1 . , 3 . , 0.25 ] , [ 1 . , 4 . , - 4.1 ] , [ 1 . , 5 . , 0 . ] ] ) wonder Intercept term came . convention linear models like ordinary squares ( OLS ) regression . suppress intercept adding term + 0 model : [ 37 ] : patsy.dmatrices ( ' y ~ x0 + x1 + 0 ' , data ) [ 1 ] [ 37 ] : DesignMatrix shape ( 5 , 2 ) x0      x1 1 0.01 2 - 0.01 3 0.25 4 - 4.10 5 0.00 Terms : ' x0 ' ( column 0 ) ' x1 ' ( column 1 ) Patsy objects passed directly algorithms like numpy.linalg.lstsq , performs ordinary squares regression : [ 38 ] : coef , resid , _ , _ = np.linalg.lstsq(X , y , rcond = ) model metadata retained design_info attribute , reattach model column names fitted coefficients obtain Series , example : [ 39 ] : coef [ 39 ] : array ( [ [ 0.3129 ] , [ - 0.0791 ] , [ - 0.2655 ] ] ) [ 40 ] : coef = pd . Series(coef.squeeze ( ) , index = X.design_info.column_names ) [ 41 ] : coef [ 41 ] : Intercept 0.312910 x0 - 0.079106 x1 - 0.265464 dtype : float64 Data Transformations Patsy Formulas mix Python code Patsy formulas ; evaluating formula , library try find functions use enclosing scope : [ 42 ] : y , X = patsy.dmatrices ( ' y ~ x0 + np.log(np.abs(x1 ) + 1 ) ' , data ) [ 43 ] : X [ 43 ] : DesignMatrix shape ( 5 , 3 ) Intercept   x0   np.log(np . abs ( x1 ) + 1 ) 1 1 0.00995 1 2 0.00995 1 3 0.22314 1 4 1.62924 1 5 0.00000 Terms : ' Intercept ' ( column 0 ) ' x0 ' ( column 1 ) ' np.log(np.abs(x1 ) + 1 ) ' ( column 2 ) commonly variable transformations include standardizing ( mean 0 variance 1 ) centering ( subtracting mean ) . Patsy built - functions purpose : [ 44 ] : y , X = patsy.dmatrices ( ' y ~ standardize(x0 ) + center(x1 ) ' , data ) [ 45 ] : X [ 45 ] : DesignMatrix shape ( 5 , 3 ) Intercept   standardize(x0 )   center(x1 ) 1 - 1.41421 0.78 1 - 0.70711 0.76 1 0.00000 1.02 1 0.70711 - 3.33 1 1.41421 0.77 Terms : ' Intercept ' ( column 0 ) ' standardize(x0 ) ' ( column 1 ) ' center(x1 ) ' ( column 2 ) modeling process , fit model dataset , evaluate model based . hold - portion new data observed later . applying transformations like center standardize , careful model form predications based new data . called stateful transformations , use statistics like mean standard deviation original dataset transforming new dataset . patsy.build_design_matrices function apply transformations new - - sample data saved information original - sample dataset : [ 46 ] : new_data = pd . DataFrame ( { .... : ' x0 ' : [ 6 , 7 , 8 , 9 ] , .... : ' x1 ' : [ 3.1 , - 0.5 , 0 , 2.3 ] , .... : ' y ' : [ 1 , 2 , 3 , 4 ] } ) [ 47 ] : new_X = patsy.build_design_matrices([X.design_info ] , new_data ) [ 48 ] : new_X [ 48 ] : [ DesignMatrix shape ( 4 , 3 ) Intercept   standardize(x0 )   center(x1 ) 1 2.12132 3.87 1 2.82843 0.27 1 3.53553 0.77 1 4.24264 3.07 Terms : ' Intercept ' ( column 0 ) ' standardize(x0 ) ' ( column 1 ) ' center(x1 ) ' ( column 2 ) ] plus symbol ( + ) context Patsy formulas mean addition , want add columns dataset , wrap special function : [ 49 ] : y , X = patsy.dmatrices ( ' y ~ I(x0 + x1 ) ' , data ) [ 50 ] : X [ 50 ] : DesignMatrix shape ( 5 , 2 ) Intercept   I(x0 + x1 ) 1 1.01 1 1.99 1 3.25 1 - 0.10 1 5.00 Terms : ' Intercept ' ( column 0 ) ' I(x0 + x1 ) ' ( column 1 ) Patsy built - transforms patsy.builtins module . online documentation . Categorical data special class transformations , explain . Categorical Data Patsy Nonnumeric data transformed model design matrix different ways . complete treatment topic outside scope book studied best course statistics . use nonnumeric terms Patsy formula , converted dummy variables default . intercept , levels left avoid collinearity : [ 51 ] : data = pd . DataFrame ( { .... : ' key1 ' : [ ' ' , ' ' , ' b ' , ' b ' , ' ' , ' b ' , ' ' , ' b ' ] , .... : ' key2 ' : [ 0 , 1 , 0 , 1 , 0 , 1 , 0 , 0 ] , .... : ' v1 ' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] , .... : ' v2 ' : [ - 1 , 0 , 2.5 , - 0.5 , 4.0 , - 1.2 , 0.2 , - 1.7 ] .... : } ) [ 52 ] : y , X = patsy.dmatrices ( ' v2 ~ key1 ' , data ) [ 53 ] : X [ 53 ] : DesignMatrix shape ( 8 , 2 ) Intercept   key1[T.b ] 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 Terms : ' Intercept ' ( column 0 ) ' key1 ' ( column 1 ) omit intercept model , columns category value included model design matrix : [ 54 ] : y , X = patsy.dmatrices ( ' v2 ~ key1 + 0 ' , data ) [ 55 ] : X [ 55 ] : DesignMatrix shape ( 8 , 2 ) key1[a ]   key1[b ] 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 Terms : ' key1 ' ( columns 0 : 2 ) Numeric columns interpreted categorical C function : [ 56 ] : y , X = patsy.dmatrices ( ' v2 ~ C(key2 ) ' , data ) [ 57 ] : X [ 57 ] : DesignMatrix shape ( 8 , 2 ) Intercept   C(key2)[T .1 ] 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 Terms : ' Intercept ' ( column 0 ) ' C(key2 ) ' ( column 1 ) multiple categorical terms model , things complicated , include interaction terms form key1 : key2 , , example , analysis variance ( ANOVA ) models : [ 58 ] : data [ ' key2 ' ] = data [ ' key2 ' ] . map ( { 0 : ' zero ' , 1 : ' ' } ) [ 59 ] : data [ 59 ] : key1   key2   v1    v2 0   zero 1 - 1.0 1    2 0.0 2 b   zero 3 2.5 3 b    4 - 0.5 4   zero 5 4.0 5 b    6 - 1.2 6   zero 7 0.2 7 b   zero 8 - 1.7 [ 60 ] : y , X = patsy.dmatrices ( ' v2 ~ key1 + key2 ' , data ) [ 61 ] : X [ 61 ] : DesignMatrix shape ( 8 , 3 ) Intercept   key1[T.b ]   key2[T.zero ] 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 Terms : ' Intercept ' ( column 0 ) ' key1 ' ( column 1 ) ' key2 ' ( column 2 ) [ 62 ] : y , X = patsy.dmatrices ( ' v2 ~ key1 + key2 + key1 : key2 ' , data ) [ 63 ] : X [ 63 ] : DesignMatrix shape ( 8 , 4 ) Intercept   key1[T.b ]   key2[T.zero ]   key1[T.b]:key2[T.zero ] 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 Terms : ' Intercept ' ( column 0 ) ' key1 ' ( column 1 ) ' key2 ' ( column 2 ) ' key1 : key2 ' ( column 3 ) Patsy provides ways transform categorical data , including transformations terms particular ordering . online documentation . 12.3 Introduction statsmodels statsmodels Python library fitting kinds statistical models , performing statistical tests , data exploration visualization . statsmodels contains " classical " frequentist statistical methods , Bayesian methods machine learning models found libraries . kinds models found statsmodels include : Linear models , generalized linear models , robust linear models Linear mixed effects models Analysis variance ( ANOVA ) methods Time series processes state space models Generalized method moments pages , use basic tools statsmodels explore use modeling interfaces Patsy formulas pandas DataFrame objects . install statsmodels Patsy discussion earlier , install : conda install statsmodels Estimating Linear Models kinds linear regression models statsmodels , basic ( e.g. , ordinary squares ) complex ( e.g. , iteratively reweighted squares ) . Linear models statsmodels different main interfaces : array based formula based . accessed API module imports : import statsmodels.api sm import statsmodels.formula.api smf use , generate linear model random data . Run following code Jupyter cell : # example reproducible rng = np.random.default_rng(seed = 12345 ) def dnorm(mean , variance , size = 1 ): isinstance ( size , int ): size = size , return mean + np.sqrt(variance ) * rng.standard_normal ( * size ) N = 100 X = np.c_[dnorm ( 0 , 0.4 , size = N ) , dnorm ( 0 , 0.6 , size = N ) , dnorm ( 0 , 0.2 , size = N ) ] eps = dnorm ( 0 , 0.1 , size = N ) beta = [ 0.1 , 0.3 , 0.5 ] y = np.dot(X , beta ) + eps , wrote " true " model known parameters beta . case , dnorm helper function generating normally distributed data particular mean variance . : [ 66 ] : X [: 5 ] [ 66 ] : array ( [ [ - 0.9005 , - 0.1894 , - 1.0279 ] , [ 0.7993 , - 1.546 , - 0.3274 ] , [ - 0.5507 , - 0.1203 , 0.3294 ] , [ - 0.1639 , 0.824 , 0.2083 ] , [ - 0.0477 , - 0.2131 , - 0.0482 ] ] ) [ 67 ] : y [: 5 ] [ 67 ] : array ( [ - 0.5995 , - 0.5885 , 0.1856 , - 0.0075 , - 0.0154 ] ) linear model generally fitted intercept term , saw Patsy . sm.add_constant function add intercept column existing matrix : [ 68 ] : X_model = sm.add_constant(X ) [ 69 ] : X_model [: 5 ] [ 69 ] : array ( [ [ 1 . , - 0.9005 , - 0.1894 , - 1.0279 ] , [ 1 . , 0.7993 , - 1.546 , - 0.3274 ] , [ 1 . , - 0.5507 , - 0.1203 , 0.3294 ] , [ 1 . , - 0.1639 , 0.824 , 0.2083 ] , [ 1 . , - 0.0477 , - 0.2131 , - 0.0482 ] ] ) sm . OLS class fit ordinary squares linear regression : [ 70 ] : model = sm . OLS(y , X ) model fit method returns regression results object containing estimated model parameters diagnostics : [ 71 ] : results = model.fit ( ) [ 72 ] : results.params [ 72 ] : array ( [ 0.0668 , 0.268 , 0.4505 ] ) summary method results print model detailing diagnostic output model : [ 73 ] : print ( results.summary ( ) ) OLS Regression Results = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = Dep . Variable :                       y    R - squared ( uncentered ): 0.469 Model :                             OLS    Adj . R - squared ( uncentered ): 0.452 Method :                  Squares    F - statistic : 28.51 Date :                 d , 12 Apr 2023 Prob ( F - statistic ): 2 . 66e-13 Time : 13 : 0 9 : 20 Log - Likelihood : - 25.611 . Observations : 100 AIC : 57.22 Df Residuals : 97 BIC : 65.04 Df Model : 3 Covariance Type :             nonrobust = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = coef     std err           t       P > | t | [ 0.025 0.975 ] ------------------------------------------------------------------------------ x1 0.0668 0.054 1.243 0.217 - 0.040 0.174 x2 0.2680 0.042 6.313 0.000 0.184 0.352 x3 0.4505 0.068 6.605 0.000 0.315 0.586 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = Omnibus : 0.435 Durbin - Watson : 1.869 Prob(Omnibus ): 0.805 Jarque - Bera ( JB ): 0.301 Skew : 0.134 Prob(JB ): 0.860 Kurtosis : 2.995 Cond . . 1.64 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = Notes : [ 1 ] R² computed centering ( uncentered ) model contai n constant . [ 2 ] Standard Errors assume covariance matrix errors correctly specified . parameter names given generic names x1 , x2 , . Suppose instead model parameters DataFrame : [ 74 ] : data = pd . DataFrame(X , columns = [ ' col0 ' , ' col1 ' , ' col2 ' ] ) [ 75 ] : data [ ' y ' ] = y [ 76 ] : data [: 5 ] [ 76 ] : col0       col1       col2          y 0 - 0.900506 - 0.189430 - 1.027870 - 0.599527 1 0.799252 - 1.545984 - 0.327397 - 0.588454 2 - 0.550655 - 0.120254 0.329359 0.185634 3 - 0.163916 0.824040 0.208275 - 0.007477 4 - 0.047651 - 0.213147 - 0.048244 - 0.015374 use statsmodels formula API Patsy formula strings : [ 77 ] : results = smf.ols ( ' y ~ col0 + col1 + col2 ' , data = data).fit ( ) [ 78 ] : results.params [ 78 ] : Intercept - 0.020799 col0 0.065813 col1 0.268970 col2 0.449419 dtype : float64 [ 79 ] : results.tvalues [ 79 ] : Intercept - 0.652501 col0 1.219768 col1 6.312369 col2 6.567428 dtype : float64 Observe statsmodels returned results Series DataFrame column names attached . need use add_constant formulas pandas objects . Given new - - sample data , compute predicted values given estimated model parameters : [ 80 ] : results.predict(data [: 5 ] ) [ 80 ] : 0 - 0.592959 1 - 0.531160 2 0.058636 3 0.283658 4 - 0.102947 dtype : float64 additional tools analysis , diagnostics , visualization linear model results statsmodels explore . kinds linear models ordinary squares . Estimating Time Series Processes class models statsmodels time series analysis . autoregressive processes , Kalman filtering state space models , multivariate autoregressive models . Let simulate time series data autoregressive structure noise . Run following Jupyter : init_x = 4 values = [ init_x , init_x ] N = 1000 b0 = 0.8 b1 = - 0.4 noise = dnorm ( 0 , 0.1 , N ) range ( N ): new_x = values [ - 1 ] * b0 + values [ - 2 ] * b1 + noise[i ] values.append(new_x ) data AR(2 ) structure ( lags ) parameters 0.8 – 0.4 . fit AR model , know number lagged terms include , fit model larger number lags : [ 82 ] : statsmodels.tsa.ar_model import AutoReg [ 83 ] : MAXLAGS = 5 [ 84 ] : model = AutoReg(values , MAXLAGS ) [ 85 ] : results = model.fit ( ) estimated parameters results intercept , estimates lags : [ 86 ] : results.params [ 86 ] : array ( [ 0.0235 , 0.8097 , - 0.4287 , - 0.0334 , 0.0427 , - 0.0567 ] ) Deeper details models interpret results cover book , plenty discover statsmodels documentation . 12.4 Introduction scikit - learn scikit - learn widely trusted general - purpose Python machine learning toolkits . contains broad selection standard supervised unsupervised machine learning methods , tools model selection evaluation , data transformation , data loading , model persistence . models classification , clustering , prediction , common tasks . install scikit - learn conda like : conda install scikit - learn excellent online print resources learning machine learning apply libraries like scikit - learn solve real - world problems . section , brief flavor scikit - learn API style . pandas integration scikit - learn improved significantly recent years , time reading improved . encourage check latest project documentation . example chapter , use - classic dataset Kaggle competition passenger survival rates Titanic 1912 . load training test datasets pandas : [ 87 ] : train = pd.read_csv ( ' datasets / titanic / train.csv ' ) [ 88 ] : test = pd.read_csv ( ' datasets / titanic / test.csv ' ) [ 89 ] : train.head ( 4 ) [ 89 ] : PassengerId   Survived   Pclass 0 1 0 3 \ 1 2 1 1 2 3 1 3 3 4 1 1      Sex    Age   SibSp 0 Braund , Mr. Owen Harris     male 22.0 1 \ 1 Cumings , Mrs. John Bradley ( Florence Briggs Thayer )   female 38.0 1 2 Heikkinen , Miss. Laina   female 26.0 0 3 Futrelle , Mrs. Jacques Heath ( Lily Peel )   female 35.0 1 Parch             Ticket      Fare Cabin Embarked 0 0 / 5 21171 7.2500 NaN         S 1 0 PC 17599 71.2833 C85         C 2 0 STON / O2 . 3101282 7.9250 NaN         S 3 0 113803 53.1000 C123         S Libraries like statsmodels scikit - learn generally fed missing data , look columns contain missing data : [ 90 ] : train.isna ( ) . sum ( ) [ 90 ] : PassengerId 0 Survived 0 Pclass 0 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype : int64 [ 91 ] : test.isna ( ) . sum ( ) [ 91 ] : PassengerId 0 Pclass 0 0 Sex 0 Age 86 SibSp 0 Parch 0 Ticket 0 Fare 1 Cabin 327 Embarked 0 dtype : int64 statistics machine learning examples like , typical task predict passenger survive based features data . model fitted training dataset evaluated - - sample testing dataset . like use Age predictor , missing data . number ways missing data imputation , simple use median training dataset fill nulls tables : [ 92 ] : impute_value = train [ ' Age ' ] .median ( ) [ 93 ] : train [ ' Age ' ] = train [ ' Age ' ] .fillna(impute_value ) [ 94 ] : test [ ' Age ' ] = test [ ' Age ' ] .fillna(impute_value ) need specify models . add column IsFemale encoded version ' Sex ' column : [ 95 ] : train [ ' IsFemale ' ] = ( train [ ' Sex ' ] = = ' female ' ) .astype ( int ) [ 96 ] : test [ ' IsFemale ' ] = ( test [ ' Sex ' ] = = ' female ' ) .astype ( int ) decide model variables create NumPy arrays : [ 97 ] : predictors = [ ' Pclass ' , ' IsFemale ' , ' Age ' ] [ 98 ] : X_train = train[predictors].to_numpy ( ) [ 99 ] : X_test = test[predictors].to_numpy ( ) [ 100 ] : y_train = train [ ' Survived ' ] .to_numpy ( ) [ 101 ] : X_train [: 5 ] [ 101 ] : array ( [ [ 3 . , 0 . , 22 . ] , [ 1 . , 1 . , 38 . ] , [ 3 . , 1 . , 26 . ] , [ 1 . , 1 . , 35 . ] , [ 3 . , 0 . , 35 . ] ] ) [ 102 ] : y_train [: 5 ] [ 102 ] : array ( [ 0 , 1 , 1 , 1 , 0 ] ) claims good model features engineered properly . use LogisticRegression model scikit - learn create model instance : [ 103 ] : sklearn.linear_model import LogisticRegression [ 104 ] : model = LogisticRegression ( ) fit model training data model fit method : [ 105 ] : model.fit(X_train , y_train ) [ 105 ] : LogisticRegression ( ) , form predictions test dataset model.predict : [ 106 ] : y_predict = model.predict(X_test ) [ 107 ] : y_predict [: 10 ] [ 107 ] : array ( [ 0 , 0 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 ] ) true values test dataset , compute accuracy percentage error metric : ( y_true = = y_predict).mean ( ) practice , additional layers complexity model training . models parameters tuned , techniques cross - validation parameter tuning avoid overfitting training data . yield better predictive performance robustness new data . Cross - validation works splitting training data simulate - - sample prediction . Based model accuracy score like mean squared error , perform grid search model parameters . models , like logistic regression , estimator classes built - cross - validation . example , LogisticRegressionCV class parameter indicating fine - grained grid search model regularization parameter C : [ 108 ] : sklearn.linear_model import LogisticRegressionCV [ 109 ] : model_cv = LogisticRegressionCV(Cs = 10 ) [ 110 ] : model_cv.fit(X_train , y_train ) [ 110 ] : LogisticRegressionCV ( ) cross - validation hand , use cross_val_score helper function , handles data splitting process . example , cross - validate model nonoverlapping splits training data , : [ 111 ] : sklearn.model_selection import cross_val_score [ 112 ] : model = LogisticRegression(C = 10 ) [ 113 ] : scores = cross_val_score(model , X_train , y_train , cv = 4 ) [ 114 ] : scores [ 114 ] : array ( [ 0.7758 , 0.7982 , 0.7758 , 0.7883 ] ) default scoring metric model dependent , possible choose explicit scoring function . Cross - validated models longer train yield better model performance . 12.5 Conclusion skimmed surface Python modeling libraries , frameworks kinds statistics machine learning implemented Python Python user interface . book focused especially data wrangling , dedicated modeling data science tools . excellent ones : Introduction Machine Learning Python Andreas Müller Sarah Guido ( O'Reilly ) Python Data Science Handbook Jake VanderPlas ( O'Reilly ) Data Science Scratch : Principles Python Joel Grus ( O'Reilly ) Python Machine Learning Sebastian Raschka Vahid Mirjalili ( Packt Publishing ) Hands - Machine Learning Scikit - Learn , Keras , TensorFlow Aurélien Géron ( O'Reilly ) books valuable resources learning , grow date underlying open source software changes . good idea familiar documentation statistics machine learning frameworks stay date latest features API . 11 Time Series 13 Data Analysis Examples Copyright 2023 , Wes McKinney . Rights Reserved .