Model Share AI - SciPy Proceedings 2024 Articles 2024 Conference Open Menu Curvenote Article SciPy Proceedings Proceedings 23rd Python Science Conference Content License : Creative Commons Attribution 4.0 International ( CC - BY-4.0 ) Credit given creator Model Share AI Integrated Toolkit Collaborative Machine Learning Model Development , Provenance Tracking , Deployment Python Heinrich Peters Michael Parrott July 10 , 2024 https://doi.org/10.25080/MDCE8355 Download PDF YouTube Video Supporting Documents Supplementary    Information Abstract Â¶ Machine learning ( ML ) revolutionizing wide range research areas industries , ML projects progress past proof - - concept stage . address problem , introduce Model Share AI ( AIMS ) , platform designed streamline collaborative model development , model provenance tracking , model deployment , host functions aiming maximize real - world impact ML research . AIMS features collaborative project spaces standardized model evaluation process ranks model submissions based performance holdout evaluation data , enabling users run experiments competitions . addition , model metadata automatically captured facilitate provenance tracking allow users learn build previous submissions . Furthermore , AIMS allows users deploy ML models built Scikit - Learn , TensorFlow Keras , PyTorch live REST APIs automatically generated web apps minimal code . ability collaboratively develop rapidly deploy models , making accessible non - technical end - users automatically generated web apps , ensures ML projects transition smoothly concept real - world application . Keywords : Machine Learning MLOps Model Deployment Provenance Tracking Crowdsourcing Â¶ 1 Introduction Â¶ Machine learning ( ML ) revolutionizing wide range research areas industries , providing data - driven solutions important societal problems . success ML projects depends effective collaboration , rigorous evaluation , ability deploy models . Traditionally , researchers practitioners version - control systems like GitHub combination custom model evaluation benchmarking experiments ensure reproducibility compare models . , systems tend lack easy - - use , structured pathways specifically designed collaboratively develop rapidly deploy ML models . Furthermore , creation custom resources model evaluation , benchmarking , deployment , require substantial upfront effort . result , models progress past proof - - concept stage deployed Davenport & Patil , 2022 Siegel , 2022 , preventing wider audience participating promise applied ML research . recent rise platforms like Hugging Face Hub Hugging Face Hub , 2023 , TensorFlow Hub TensorFlow Hub , 2023 , ML flow MLflow , 2023 Chen et al . , 2020 Zaharia et al . , 2018 , illustrates demand open - source model repositories ML Ops solutions , barriers entry high researchers , educators , practitioners non - technical disciplines . Model Share AI ( AIMS ) addresses problem providing lightweight , easy - - use alternative . lines code , users create Model Playgrounds - standardized ML project spaces offer - - toolkit collaborative model improvement , experiment tracking , model metadata analytics , instant model deployment , allowing researchers rapidly iterate ML models streamlined workflow . present paper provides introduction AIMS situating relation current solutions outlining key functions , technical background , workflow . 2 Related Work Â¶ primary objective AIMS offer easy pathway organize ML projects reducing common complex tasks minimal code . approach builds work projects focused providing value common ML tasks ( e.g. , model improvement , version tracking , deployment , etc . ) . Currently , open - source tools platforms , Hugging Face Hub Hugging Face Hub , 2023 , TensorFlow Hub TensorFlow Hub , 2023 , ML flow MLflow , 2023 Chen et al . , 2020 Zaharia et al . , 2018 , Open ML Feurer et al . , 2021 Vanschoren et al . , 2014 Rijn et al . , 2013 , providing ML model repositories researchers find download model objects deploy models . Hugging Face Hub Hugging Face Hub , 2023 platform allowing users share pre - trained models , datasets , demos ML projects . GitHub - inspired features code - sharing collaboration , discussions pull requests , includes pathway model deployment API endpoints . Similarly , TensorFlow Hub TensorFlow Hub , 2023 repository library reusable ML TensorFlow , enabling users fine - tune deploy deep learning models . Deployment facilitated TensorFlow Serving Olston et al . , 2017 , allows users models accessible server API endpoints . ML flow Chen et al . , 2020 MLflow , 2023 Zaharia et al . , 2018 open - source platform manages end - - end ML lifecycle . provides experiment tracking , code packaging , model registry , model serving , integration popular ML frameworks . Hugging Face Hub , TensorFlow Hub , ML flow suited large - scale deep learning tasks , Open ML Feurer et al . , 2021 Vanschoren et al . , 2014 Rijn et al . , 2013 , focuses classic ML model reproducibility . , researchers share , explore , experiment ML models , datasets , workflows , currently option model deployment . Open ML API provides access programming languages , users employ Open ML web interface browse visualize data , models , experiments . Hugging Face Hub TensorFlow Hub primarily model - focused platforms , providing repositories pre - trained models users fine - tune specific purposes . Open ML ML flow , hand , task - focused , emphasizing model evaluation benchmarking standardized tasks . platforms extensively ML researchers practitioners , including industry - scale projects , wide range features customizations overwhelming researchers non - technical disciplines , students , educators . comparison , AIMS stands prioritizing ease use hyper - collaborative approach . Unlike Hugging Face Hub TensorFlow Hub , AIMS emphasizes model metadata analytics task - focused collaboration . Compared ML flow , offers community - based features like competitions promote collective problem - solving crowd - sourcing . , Open ML excels model evaluation benchmarking , AIMS provides additional capabilities model deployment , allowing users share models directly local environment live REST APIs auto - generated web applications . Taken , key distinctions AIMS beginner - friendly design strong focus collaborative model development , goal providing value ML researchers researchers , practitioners , educators non - technical disciplines . 3 Model Share AI Â¶ AIMS provides standardized ML project spaces ( Model Playgrounds ; FigureÂ 1 ) accessible ML Ops features designed support collaborative model development , model metadata analytics , model deployment , host functions aiming maximize real - world impact ML research . , simplifies , combines , extends capabilities current solutions 4 important ways : Collaborative model development : AIMS facilitates collaborative model development crowd - sourcing standardized model evaluation procedures , experiment tracking , competitions . Model registry : AIMS hosts model objects , model metadata , automatically extracted model submission , enabling users analyze modeling decisions lead high performance specific tasks . Model deployment : Model deployment simplified considerably , allowing users share models live REST APIs pre - built web apps directly Python training environments lines code . AIMS provides wide range supporting functionalities , including workflows reproducibility , data sharing , code sharing , creating ML project portfolios , . FigureÂ 1 : Model Playground standardized ML project space , representing ML task associated specific dataset task type ( classification , regression ) . Model Playground includes Experiments Competitions collaborative model development , model registry model metadata user - generated code , deployment functionalities including live REST API pre - built web - apps . 3.1 Key Functions Â¶ 3.1.1 Collaborative Model Development Â¶ key feature AIMS focus collaborative model development crowd - sourced model improvement , enabling teams iterate quickly allowing collaborators build otherâs progress , libraries . supervised learning tasks , users collaboratively submit models Experiments Competitions associated Model Playground project order track model performance rank submissions standardized leaderboards according evaluation metric choice . Experiments Competitions set providing holdout evaluation data predictions submitted models evaluated . Standardized model evaluations allow collaborators track performance models wide range model metadata automatically extracted submitted models added model registry ( section ) . box , AIMS calculates accuracy , f1 - score , precision , recall classification tasks , mean squared error , root mean squared error , mean absolute error , R 2 R^{2 } R 2 -scores regression tasks . main difference Experiments Competitions proportion evaluation data kept secret Competitions , preventing participants deliberately overfitting evaluation data . able submit models shared Experiments enables ML teams standardize tasks , rigorously track progress , build otherâs success , Competitions facilitate crowd - sourced solutions . Experiments Competitions public ( AIMS user submit ) private ( designated team members submit ) . Users deploy model Experiment Competition REST API associated Model Playground single line code . 3.1.2 Model Registry Â¶ Model versions available Model Playground comprehensive model metadata automatically extracted submitted model . addition evaluation metrics , includes hyperparameter settings Scikit - Learn models model architecture data ( layer types dimensions , number parameters , optimizers , loss function , memory size ) Keras Pytorch models . Users submit additional metadata choose capture . Model metadata integrated Competition Experiment leaderboards , enabling users analyze types models tend perform specific ML task . Users visually explore leaderboards Model Playground page download leaderboards Pandas data frames run analyses . set AIMS methods designed visualize model metadata . example , models compared color - coded layout showing differences model architectures hyperparameter settings . Furthermore , users instantiate models AIMS model registry reproducible environments . Taken , functions designed streamline management , collaboration , deployment ML models , enhancing discoverability , reproducibility , traceability lifecycle . 3.1.3 Instant Model Deployment Â¶ AIMS currently allows users deploy ML models built Scikit - Learn Pedregosa et al . , 2011 , Tensorflow Keras Abadi et al . , 2016 Chollet , 2015 , Pytorch Paszke et al . , 2019 live REST APIs rapidly minimal code . Additionally , users deploy models ML frameworks transforming ONNX ( Open Neural Network Exchange ) format - open - source format standardized representations ML models goal making interoperable platforms . deployed model associated Model Playground page AIMS website REST API endpoint hosted serverless AWS backend . End - users manually upload data predictions automatically generated web app Model Playground Page , programmatically query REST API associated model . addition auto - generated web apps , AIMS enables users submit Streamlit apps . box , AIMS supports models built tabular , text , image , audio , video data . Allowing users deploy models minimal effort making models accessible non - technical end - users web apps holds promise making ML research applicable real - world challenges . FigureÂ 2 : Overview AIMS architecture . AIMS Python library allows users create Model Playground pages , submit deploy models , analyze model metadata . modelshare.ai website provides graphical user interface explore model metadata generate predictions auto - generated web apps . required resources automatically generated scalable serverless cloud infrastructure . 3.2 Architecture Â¶ AIMS consists main components : open - source Python library , user - owned cloud backend resources , AIMS website ( FigureÂ 2 ) . AIMS Python library main interface allowing users set Model Playground pages ( including Experiments Competitions ) , submit deploy models , analyze model metadata , reproduce model artifacts . provides accessible layer facilitates creation cloud backend resources power REST APIs , model evaluations model metadata extraction . ModelPlayground ( ) class acts local representation Model Playground page associated REST API . provides range methods configure , change , query Model Playground resources . detailed overview Python library provided ( AIMS Workflow ) . cloud backend hosts model objects associated artifacts S3 storage , REST APIs deployed serverless lambda functions . Lambda functions programs scripts run high - availability AWS compute infrastructure . invoked event sources ( e.g. , API calls ) scale automatically based volume incoming requests . means users maintain servers , pay time resources actually consumed , idle time . AIMS Python library allows users automatically generate deploy lambda functions based specific properties ML tasks , need explicitly manage AWS resources . important lambda functions context AIMS Evaluation Lambda , computes evaluation metrics extracts model metadata submitted models , Main Lambda , computes predictions data submitted REST API . Runtime models automatically packaged Docker containers run lambda . Additionally , certain metadata stored centralized Redis database powers modelshare.ai website . AIMS website hosts user profile pages , model pages , web apps , example code , documentation page , user - generated code documentation specific ML projects ( Supplementary Information - E ) . 3.3 AIMS Workflow Â¶ AIMS workflow designed help teams collaboratively continuously train , evaluate , improve , select , deploy models standardized ML project spaces Model Playgrounds . training model , users submit model Competition Experiment associated Model Playground . model automatically evaluated , model metadata extracted . Evaluations metadata available graphical user interface Model Playground page queried AIMS Python library , enabling users analyze types models perform given learning task . information improve training process submit models select model instantly deploy single line code . Deployed models easily swapped degrade time outperformed new submissions . overview process found FigureÂ 3 . FigureÂ 3 : ML Ops workflow AIMS . Users iteratively train , submit , evaluate , analyze models . contributors access model evaluations , model architecture metadata , reproducible model objects previous submissions , rapidly develop high - performing models . Submitted models easily deployed live REST APIs . Deployed runtime models monitored seamlessly swapped newly submitted models . order use AIMS , users need create AIMS user profile generate credentials AIMS website . Additionally , users required generate AWS credentials wish deploy models AWS resources . AIMS workflow centered concept Model Playgrounds . Model Playground standardized ML project space , representing ML project associated specific dataset task type , classification regression . Model Playground instantiated locally ModelPlayground ( ) class AIMS Python library . , users need specify input_type ( tabular , text , image , audio , etc . ) task_type ( classification , regression ) , select Model Playground public private . Private Model Playgrounds accessed invited collaborators , public Model Playgrounds open AIMS users . instantiating Model Playground object , users create online Model Playground page calling create ( ) method submitting evaluation data . generates fully functioning Model Playground Page AIMS website , including placeholder REST API , enables users submit models associated Experiments Competitions . Model Playground Page created , users start submitting models Experiments Competitions deploy models Model Playgroundâs REST API . model submission includes model object ( Scikit - Learn , Keras , PyTorch , ONNX ) , preprocessor function , set predicted values corresponding previously submitted evaluation data . predictions evaluated evaluation data , model metadata automatically extracted model objects . Users submit additional metadata dictionary format custom_metadata argument submit_model ( ) method . submitting models , users explore evaluations model metadata associated Model Playground page query information analysis get_leaderboard ( ) compare_models ( ) methods . inspect improve models , users instantiate models leaderboard instantiate_model ( ) method . Alternatively , users instantly deploy model deploy_model ( ) method referring modelâs leaderboard version number . Additionally , users submit example data , help end - users format input data correctly , y training data help web app prediction API transform raw model outputs correct labels . mentioned , process stop model deployed . Users submit analyze models , informed previous submissions , easily monitor swap runtime model update_runtime_model ( ) method . overview model deployment process , including code , available FigureÂ 4 . Detailed tutorials documentation found Supplementary Information A. FigureÂ 4 : Model deployment process : Models deployed lines code . instantiating local Model Playground object , users create Model Playground page ready model submissions . Submitted models automatically evaluated deployed live REST APIs . 4 Impact Â¶ Collaborative model improvement crowd - sourcing important , teams efficient foster diverse perspectives . collective efforts contribute democratization ML , provide access resources wider audience , enable community - driven innovation . instance , AIMS Competition feature facilitate crowd - sourced research projects disciplines utilizing common task framework . Relatedly , AIMS model registry promotes discoverability , transparency , reproducibility providing centralized platform users find models associated metadata . Sharing model metadata , model architectures , hyperparameters , training data , evaluation metrics , allows researchers verify previous results build otherâs work . repository acts educational resource , offering students , educators , self - learners opportunity study ML models , techniques , best practices . example , AIMS far extensively classrooms Columbia University organize challenges , collaboratively work ML projects , models accessible API endpoints . addition , AIMS positioned realize important missions regard model deployment . Firstly , simplifying model deployment important lowers barriers entry , allowing developers , researchers , organizations incorporate ML projects . Secondly , easier deployment saves resources time , developers dedicate effort model training , tuning , evaluation . Ultimately , streamlined deployment process allows users iterate faster explore novel ideas real - world contexts . 5 Future Work Â¶ AIMS provides simple approach collaborative model development deployment , opportunities improvements . Firstly , important strike right balance flexibility ease use . highly flexible platform allow researchers use ML frameworks libraries , accommodate wide range data sources formats , support custom deployment strategies . , flexibility come cost increased complexity harder learning curve users . hand , user - friendly solution provide simpler , streamlined interface lack flexibility accommodate unique complex requirements . default , AIMS prioritizes standardization ease use , making attractive researchers , educators , data scientists interested quick lightweight solutions . users want flexibility , AIMS provides customizations , custom AWS lambda functions , custom containers , custom metadata submissions . continue extend functionality AIMS useful wide range users applications . includes making platform compatible advanced model types , task types , ML frameworks ( e.g. , PySpark ) . Relatedly , future work include additional ML Ops functionality , including improved pathways monitoring , model explainability , continuous retraining , automated ML ( Auto ML ) Hutter et al . , 2019 et al . , 2021 . point special interest , new model submission contributes growing repository evaluated , reusable ML models , including rich model metadata . resources utilized suggest pre - trained models expected work given task enable users run analyses choose pre - trained models . hope AIMS resource researchers interested meta - learning Hospedales et al . , 2020 Finn et al . , 2017 related problems . Additionally , expect AIMS increasingly researchers disciplines , including social sciences natural sciences , trying solve substantive questions ML . planning accommodate diverse needs order promote cross - fertilization widespread participation promise applied ML research . 6 Conclusion Â¶ AIMS provides versatile easy - - use approach collaborative model development , model metadata analytics , model deployment . enables users quickly find , analyze , collaboratively improve trained ML models , share models local environment directly live REST APIs pre - built web applications single tightly integrated workflow . Compared existing solutions , AIMS intended lower barriers entry ML research , making attractive large group researchers , educators , data scientists . confident AIMS widely tool maximize real - world impact ML research . Acknowledgments Â¶ M.P. developed original idea Model Share AI platform 2019 , assembled team , led way platformâs completion chief engineer team lead . H.P. contributed development AIMS Python library pivotal impact ideation development 2020 . profoundly grateful early repeated organizational backers . Columbia Universityâs QMSS program Director Greg Eirich early supporters . Professor David Park , Columbia Universityâs GSAS Dean Strategic Initiatives , helped carve strategy led projectâs eventual fundraising success . special acknowledgment goes Columbia Universityâs ISERP Startup Center . funding enabled Dr. Michael Parrott , founding Director , assemble early team develop prototype platform . early funding technical prototype laid groundwork large - scale funding support Alfred P. Sloan Foundation . Director Josh Greenburg Tech program team Sloan gave momentum repeated support innovate build research education - facing ML Ops platform . grant resources belief success , project impossible . course , resources starting point . highlight exceptional contributions Sam Alsmadi , developed AIMS website , Gretchen Street , led way library user security , ML datasets , documentation . Finally , want thank talented alumni contributed code years . platform possible hard work innovation ! References Â¶ Davenport , T. H. , & Patil , D. J. ( 2022 ) . Data Scientist Sexiest Job 21st Century ? Harvard Business Review . https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century Siegel , E. ( 2022 ) . Models Rarely Deployed : Industry - wide Failure Machine Learning Leadership . https://www.kdnuggets.com/models-are-rarely-deployed-an-industry-wide-failure-in-machine-learning-leadership.html Hugging Face Hub . ( 2023 ) . https://huggingface.co/docs/hub/index TensorFlow Hub . ( 2023 ) . https://www.tensorflow.org/hub MLflow . ( 2023 ) . https://mlflow.org/ Chen , A. , Chow , A. , Davidson , A. , DCunha , A. , Ghodsi , A. , Hong , S. A. , Konwinski , A. , Mewald , C. , Murching , S. , Nykodym , T. , Ogilvie , P. , Parkhe , M. , Singh , A. , Xie , F. , Zaharia , M. , Zang , R. , Zheng , J. , & Zumar , C. ( 2020 ) . Developments MLflow : System Accelerate Machine Learning Lifecycle . Proceedings Fourth International Workshop Data Management End - - End Machine Learning , 1â4 . 10.1145/3399579.3399867 Zaharia , M. , Chen , A. , Davidson , A. , Ghodsi , A. , Hong , S. , Konwinski , A. , Murching , S. , Nykodym , T. , Ogilvie , P. , Parkhe , M. , Xie , F. , & Zumar , C. ( 2018 ) . Accelerating Machine Learning Lifecycle MLflow . IEEE Data Eng . Bull . https://www.semanticscholar.org/paper/Accelerating-the-Machine-Learning-Lifecycle-with-Zaharia-Chen/b2e0b79e6f180af2e0e559f2b1faba66b2bd578a Feurer , M. , Van Rijn , J. N. , Kadra , A. , Gijsbers , P. , Mallik , N. , Ravi , S. , MÃ¼ller , A. , Vanschoren , J. , & Hutter , F. ( 2021 ) . Openml - python : extensible python api openml . Journal Machine Learning Research , 22 ( 1 ) , 4573â4577 . Vanschoren , J. , van Rijn , J. N. , Bischl , B. , & Torgo , L. ( 2014 ) . OpenML : networked science machine learning . ACM SIGKDD Explorations Newsletter , 15 ( 2 ) , 49â60 . 10.1145/2641190.2641198 van Rijn , J. N. , Bischl , B. , Torgo , L. , Gao , B. , Umaashankar , V. , Fischer , S. , Winter , P. , Wiswedel , B. , Berthold , M. R. , & Vanschoren , J. ( 2013 ) . OpenML : Collaborative Science Platform . H. Blockeel , K. Kersting , S. Nijssen , & F. Å½eleznÃ½ ( Eds . ) , Machine Learning Knowledge Discovery Databases ( pp . 645â649 ) . Springer . 10.1007/978 - 3 - 642 - 40994 - 3_46 Olston , C. , Fiedel , N. , Gorovoy , K. , Harmsen , J. , Lao , L. , Li , F. , Rajashekhar , V. , Ramesh , S. , & Soyke , J. ( 2017 ) . TensorFlow - Serving : Flexible , High - Performance ML Serving . arXiv . 10.48550 / arXiv.1712.06139 Pedregosa , F. , Varoquaux , G. , Gramfort , A. , Michel , V. , Thirion , B. , Grisel , O. , Blondel , M. , Prettenhofer , P. , Weiss , R. , Dubourg , V. , Vanderplas , J. , Passos , A. , Cournapeau , D. , Brucher , M. , Perrot , M. , & Duchesnay , Ã. ( 2011 ) . Scikit - learn : Machine Learning Python . Journal Machine Learning Research , 12 , 2825â2830 . http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html Abadi , M. , Barham , P. , Chen , J. , Chen , Z. , Davis , A. , Dean , J. , Devin , M. , Ghemawat , S. , Irving , G. , Isard , M. , Kudlur , M. , Levenberg , J. , Monga , R. , Moore , S. , Murray , D. G. , Steiner , B. , Tucker , P. , Vasudevan , V. , Warden , P. , â ¦ Zheng , X. ( 2016 ) . TensorFlow : system large - scale machine learning . https://doi.org/10.48550/arXiv.1605.08695 Chollet , F. ( 2015 ) . Keras : Python deep learning API . https://keras.io/ Paszke , A. , Gross , S. , Massa , F. , Lerer , A. , Bradbury , J. , Chanan , G. , Killeen , T. , Lin , Z. , Gimelshein , N. , Antiga , L. , Desmaison , A. , KÃ¶pf , A. , Yang , E. , DeVito , Z. , Raison , M. , Tejani , A. , Chilamkurthy , S. , Steiner , B. , Fang , L. , â ¦ Chintala , S. ( 2019 ) . PyTorch : Imperative Style , High - Performance Deep Learning Library . arXiv . 10.48550 / arXiv.1912.01703 19 references Model Share AI Supplementary    Information SciPy Proceedings Home Past Conferences Submit Article Author Instructions Curvenote Terms & Privacy