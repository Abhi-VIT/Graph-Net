Optimization root finding ( scipy.optimize ) — SciPy v1.15.2 Manual Skip main content Ctrl + K SciPy Installing User Guide API reference Building source Development Release notes GitHub Twitter Installing User Guide API reference Building source Development Release notes GitHub Twitter Section Navigation scipy scipy.cluster scipy.constants scipy.datasets scipy.differentiate scipy.fft scipy.fftpack scipy.integrate scipy.interpolate scipy.io scipy.linalg scipy.ndimage scipy.odr scipy.optimize scipy.signal scipy.sparse scipy.spatial scipy.special scipy.stats SciPy API Optimization ... Optimization root finding ( scipy.optimize ) # SciPy optimize provides functions minimizing ( maximizing ) 
 objective functions , possibly subject constraints . includes 
 solvers nonlinear problems ( support local global 
 optimization algorithms ) , linear programming , constrained 
 nonlinear - squares , root finding , curve fitting . Common functions objects , shared different solvers , : show_options ( [ solver , Â   method , Â   disp ] ) documentation additional options optimization solvers . OptimizeResult Represents optimization result . OptimizeWarning Optimization # Scalar functions optimization # minimize_scalar ( fun[,Â   bracket , Â   bounds , Â   ... ] ) Local minimization scalar function variable . minimize_scalar function supports following methods : minimize_scalar(method = âbrentâ ) minimize_scalar(method = âboundedâ ) minimize_scalar(method = âgoldenâ ) Local ( multivariate ) optimization # minimize ( fun , Â   x0[,Â   args , Â   method , Â   jac , Â   hess , Â   ... ] ) Minimization scalar function variables . minimize function supports following methods : minimize(method = âNelder - Meadâ ) minimize(method = âPowellâ ) minimize(method = âCGâ ) minimize(method = âBFGSâ ) minimize(method = âNewton - CGâ ) minimize(method = âL - BFGS - Bâ ) minimize(method = âTNCâ ) minimize(method = âCOBYLAâ ) minimize(method = âCOBYQAâ ) minimize(method = âSLSQPâ ) minimize(method = âtrust - constrâ ) minimize(method = âdoglegâ ) minimize(method = âtrust - ncgâ ) minimize(method = âtrust - krylovâ ) minimize(method = âtrust - exactâ ) Constraints passed minimize function single object 
 list objects following classes : NonlinearConstraint ( fun , Â   lb , Â   ub[,Â   jac , Â   ... ] ) Nonlinear constraint variables . LinearConstraint ( A[,Â   lb , Â   ub , Â   keep_feasible ] ) Linear constraint variables . Simple bound constraints handled separately special class 
 : Bounds ( [ lb , Â   ub , Â   keep_feasible ] ) Bounds constraint variables . Quasi - Newton strategies implementing HessianUpdateStrategy interface approximate Hessian minimize function ( available âtrust - constrâ method ) . Available 
 quasi - Newton methods implementing interface : BFGS ( [ exception_strategy , Â   min_curvature , Â   ... ] ) Broyden - Fletcher - Goldfarb - Shanno ( BFGS ) Hessian update strategy . SR1 ( [ min_denominator , Â   init_scale ] ) Symmetric - rank-1 Hessian update strategy . Global optimization # basinhopping ( func , Â   x0[,Â   niter , Â   T , Â   stepsize , Â   ... ] ) Find global minimum function basin - hopping algorithm . brute ( func , Â   ranges[,Â   args , Â   Ns , Â   full_output , Â   ... ] ) Minimize function given range brute force . differential_evolution ( func , Â   bounds[,Â   args , Â   ... ] ) Finds global minimum multivariate function . shgo ( func , Â   bounds[,Â   args , Â   constraints , Â   n , Â   ... ] ) Finds global minimum function SHG optimization . dual_annealing ( func , Â   bounds[,Â   args , Â   ... ] ) Find global minimum function Dual Annealing . direct ( func , Â   bounds , Â   * [ , Â   args , Â   eps , Â   maxfun , Â   ... ] ) Finds global minimum function DIRECT algorithm . - squares curve fitting # Nonlinear - squares # least_squares ( fun , Â   x0[,Â   jac , Â   bounds , Â   ... ] ) Solve nonlinear - squares problem bounds variables . Linear - squares # nnls ( , Â   b[,Â   maxiter , Â   atol ] ) Solve argmin_x || Ax - b ||_2 x>=0 . lsq_linear ( , Â   b[,Â   bounds , Â   method , Â   tol , Â   ... ] ) Solve linear - squares problem bounds variables . isotonic_regression ( y , Â   * [ , Â   weights , Â   increasing ] ) Nonparametric isotonic regression . Curve fitting # curve_fit ( f , Â   xdata , Â   ydata[,Â   p0,Â   sigma , Â   ... ] ) Use non - linear squares fit function , f , data . Root finding # Scalar functions # root_scalar ( f[,Â   args , Â   method , Â   bracket , Â   ... ] ) Find root scalar function . brentq ( f , Â   , Â   b[,Â   args , Â   xtol , Â   rtol , Â   maxiter , Â   ... ] ) Find root function bracketing interval Brent method . brenth ( f , Â   , Â   b[,Â   args , Â   xtol , Â   rtol , Â   maxiter , Â   ... ] ) Find root function bracketing interval Brent method hyperbolic extrapolation . ridder ( f , Â   , Â   b[,Â   args , Â   xtol , Â   rtol , Â   maxiter , Â   ... ] ) Find root function interval Ridder method . bisect ( f , Â   , Â   b[,Â   args , Â   xtol , Â   rtol , Â   maxiter , Â   ... ] ) Find root function interval bisection . newton ( func , Â   x0[,Â   fprime , Â   args , Â   tol , Â   ... ] ) Find root real complex function Newton - Raphson ( secant Halley ) method . toms748 ( f , Â   , Â   b[,Â   args , Â   k , Â   xtol , Â   rtol , Â   ... ] ) Find root TOMS Algorithm 748 method . RootResults ( root , Â   iterations , Â   ... ) Represents root finding result . root_scalar function supports following methods : root_scalar(method = âbrentqâ ) root_scalar(method = âbrenthâ ) root_scalar(method = âbisectâ ) root_scalar(method = âridderâ ) root_scalar(method = ânewtonâ ) root_scalar(method = âtoms748â ) root_scalar(method = âsecantâ ) root_scalar(method = âhalleyâ ) table lists situations appropriate methods , asymptotic convergence rates iteration ( function evaluation ) 
 successful convergence simple root ( * ) . 
 Bisection slowest , adding bit accuracy 
 function evaluation , guaranteed converge . 
 bracketing methods ( eventually ) increase number accurate 
 bits 50 % function evaluation . 
 derivative - based methods , built newton , converge quickly 
 initial value close root .   applied 
 functions defined ( subset ) complex plane . Domain f Bracket ? Derivatives ? Solvers Convergence fprime fprime2 Guaranteed ? Rate(s ) ( * ) R Yes N / N / bisection brentq brenth ridder toms748 Yes Yes Yes Yes Yes 1 âLinearâ > = 1 , < = 1.62 > = 1 , < = 1.62 2.0 ( 1.41 ) 2.7 ( 1.65 ) R C secant 1.62 ( 1.62 ) R C Yes newton 2.00 ( 1.41 ) R C Yes Yes halley 3.00 ( 1.44 ) scipy.optimize.cython_optimize â Typed Cython versions root finding functions Fixed point finding : fixed_point ( func , Â   x0[,Â   args , Â   xtol , Â   maxiter , Â   ... ] ) Find fixed point function . Multidimensional # root ( fun , Â   x0[,Â   args , Â   method , Â   jac , Â   tol , Â   ... ] ) Find root vector function . root function supports following methods : root(method = âhybrâ ) root(method = âlmâ ) root(method = âbroyden1â ) root(method = âbroyden2â ) root(method = âandersonâ ) root(method = âlinearmixingâ ) root(method = âdiagbroydenâ ) root(method = âexcitingmixingâ ) root(method = âkrylovâ ) root(method = âdf - saneâ ) Elementwise Minimization Root Finding # Elementwise Scalar Optimization ( scipy.optimize.elementwise ) Root finding find_root bracket_root Minimization find_minimum bracket_minimum Linear programming / MILP # milp ( c , Â   * [ , Â   integrality , Â   bounds , Â   ... ] ) Mixed - integer linear programming linprog ( c[,Â   A_ub , Â   b_ub , Â   A_eq , Â   b_eq , Â   bounds , Â   ... ] ) Linear programming : minimize linear objective function subject linear equality inequality constraints . linprog function supports following methods : linprog(method = âsimplexâ ) linprog(method = âinterior - pointâ ) linprog(method = ârevised simplexâ ) linprog(method = âhighs - ipmâ ) linprog(method = âhighs - dsâ ) linprog(method = âhighsâ ) simplex , interior - point , revised simplex methods support callback 
 functions , : linprog_verbose_callback ( res ) sample callback function demonstrating linprog callback interface . Assignment problems # linear_sum_assignment Solve linear sum assignment problem . quadratic_assignment ( , Â   B[,Â   method , Â   options ] ) Approximates solution quadratic assignment problem graph matching problem . quadratic_assignment function supports following methods : quadratic_assignment(method = âfaqâ ) quadratic_assignment(method = â2optâ ) Utilities # Finite - difference approximation # approx_fprime ( xk , Â   f[,Â   epsilon ] ) Finite difference approximation derivatives scalar vector - valued function . check_grad ( func , Â   grad , Â   x0,Â   * args[,Â   epsilon , Â   ... ] ) Check correctness gradient function comparing ( forward ) finite - difference approximation gradient . Line search # bracket ( func[,Â   xa , Â   xb , Â   args , Â   grow_limit , Â   ... ] ) Bracket minimum function . line_search ( f , Â   myfprime , Â   xk , Â   pk[,Â   gfk , Â   ... ] ) Find alpha satisfies strong Wolfe conditions . Hessian approximation # LbfgsInvHessProduct ( * args , Â   * * kwargs ) Linear operator L - BFGS approximate inverse Hessian . HessianUpdateStrategy ( ) Interface implementing Hessian update strategies . Benchmark problems # rosen ( x ) Rosenbrock function . rosen_der ( x ) derivative ( i.e. gradient ) Rosenbrock function . rosen_hess ( x ) Hessian matrix Rosenbrock function . rosen_hess_prod ( x , Â   p ) Product Hessian matrix Rosenbrock function vector . Legacy functions # functions recommended use new scripts ; 
 methods accessible newer , consistent 
 interfaces , provided interfaces . Optimization # General - purpose multivariate methods : fmin ( func , Â   x0[,Â   args , Â   xtol , Â   ftol , Â   maxiter , Â   ... ] ) Minimize function downhill simplex algorithm . fmin_powell ( func , Â   x0[,Â   args , Â   xtol , Â   ftol , Â   ... ] ) Minimize function modified Powell method . fmin_cg ( f , Â   x0[,Â   fprime , Â   args , Â   gtol , Â   norm , Â   ... ] ) Minimize function nonlinear conjugate gradient algorithm . fmin_bfgs ( f , Â   x0[,Â   fprime , Â   args , Â   gtol , Â   norm , Â   ... ] ) Minimize function BFGS algorithm . fmin_ncg ( f , Â   x0,Â   fprime[,Â   fhess_p , Â   fhess , Â   ... ] ) Unconstrained minimization function Newton - CG method . Constrained multivariate methods : fmin_l_bfgs_b ( func , Â   x0[,Â   fprime , Â   args , Â   ... ] ) Minimize function func L - BFGS - B algorithm . fmin_tnc ( func , Â   x0[,Â   fprime , Â   args , Â   ... ] ) Minimize function variables subject bounds , gradient information truncated Newton algorithm . fmin_cobyla ( func , Â   x0,Â   cons[,Â   args , Â   ... ] ) Minimize function Constrained Optimization Linear Approximation ( COBYLA ) method . fmin_slsqp ( func , Â   x0[,Â   eqcons , Â   f_eqcons , Â   ... ] ) Minimize function Sequential Squares Programming Univariate ( scalar ) minimization methods : fminbound ( func , Â   x1,Â   x2[,Â   args , Â   xtol , Â   ... ] ) Bounded minimization scalar functions . brent ( func[,Â   args , Â   brack , Â   tol , Â   full_output , Â   ... ] ) Given function variable possible bracket , return local minimizer function isolated fractional precision tol . golden ( func[,Â   args , Â   brack , Â   tol , Â   ... ] ) Return minimizer function variable golden section method . - squares # leastsq ( func , Â   x0[,Â   args , Â   Dfun , Â   full_output , Â   ... ] ) Minimize sum squares set equations . Root finding # General nonlinear solvers : fsolve ( func , Â   x0[,Â   args , Â   fprime , Â   ... ] ) Find roots function . broyden1 ( F , Â   xin[,Â   iter , Â   alpha , Â   ... ] ) Find root function , Broyden Jacobian approximation . broyden2 ( F , Â   xin[,Â   iter , Â   alpha , Â   ... ] ) Find root function , Broyden second Jacobian approximation . NoConvergence Exception raised nonlinear solver fails converge specified maxiter . Large - scale nonlinear solvers : newton_krylov ( F , Â   xin[,Â   iter , Â   rdiff , Â   method , Â   ... ] ) Find root function , Krylov approximation inverse Jacobian . anderson ( F , Â   xin[,Â   iter , Â   alpha , Â   w0,Â   M , Â   ... ] ) Find root function , ( extended ) Anderson mixing . BroydenFirst ( [ alpha , Â   reduction_method , Â   max_rank ] ) Find root function , Broyden Jacobian approximation . InverseJacobian ( jacobian ) simple wrapper inverts Jacobian solve method . KrylovJacobian ( [ rdiff , Â   method , Â   ... ] ) Find root function , Krylov approximation inverse Jacobian . Simple iteration solvers : excitingmixing ( F , Â   xin[,Â   iter , Â   alpha , Â   ... ] ) Find root function , tuned diagonal Jacobian approximation . linearmixing ( F , Â   xin[,Â   iter , Â   alpha , Â   verbose , Â   ... ] ) Find root function , scalar Jacobian approximation . diagbroyden ( F , Â   xin[,Â   iter , Â   alpha , Â   verbose , Â   ... ] ) Find root function , diagonal Broyden Jacobian approximation . previous scipy.odr.quadratic Cython optimize root finding API page Optimization Scalar functions optimization Local ( multivariate ) optimization Global optimization - squares curve fitting Nonlinear - squares Linear - squares Curve fitting Root finding Scalar functions Multidimensional Elementwise Minimization Root Finding Linear programming / MILP Assignment problems Utilities Finite - difference approximation Line search Hessian approximation Benchmark problems Legacy functions Optimization - squares Root finding Â © Copyright 2008 - 2025 , SciPy community . Created Sphinx 7.3.7 . Built PyData Sphinx Theme 0.15.2 .